{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks Homework \n",
    "***\n",
    "**Name**: $<$Sayali Sonawane$>$ \n",
    "***\n",
    "\n",
    "This assignment is due on Moodle by **11:59pm on Wednesday May 2nd**. Submit only this Jupyter notebook to Moodle.  Do not compress it using tar, rar, zip, etc. Your solutions to analysis questions should be done in Markdown directly below the associated question.  Remember that you are encouraged to discuss the problems with your instructors and classmates, but **you must write all code and solutions on your own**.  For a refresher on the course **Collaboration Policy** click [here](https://github.com/chrisketelsen/CSCI5622-Machine-Learning/blob/master/resources/syllabus.md#collaboration-policy)\n",
    "\n",
    "**NOTES**: \n",
    "\n",
    "- Do **NOT** load or use any Python packages that are not available in Anaconda 3.6. \n",
    "- Some problems with code may be autograded.  If we provide a function API **do not** change it.  If we do not provide a function API then you're free to structure your code however you like. \n",
    "- Submit only this Jupyter notebook to Moodle.  Do not compress it using tar, rar, zip, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-20T02:42:13.096354Z",
     "start_time": "2018-04-20T02:42:12.091124Z"
    },
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pickle, gzip\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [30 points] Problem 1: Building and Training a Feed-Forward Neural Network \n",
    "***\n",
    "\n",
    "In this problem you'll implement a general feed-forward neural network class that utilizes sigmoid activation functions. Your tasks will be to implement `forward propagation`, `prediction`, `back propagation`, `gradient_checking`, and a general `train` routine to learn the weights in your network via Stochastic Gradient Descent.  \n",
    "\n",
    "The skeleton for the `Network` class is below. Note that this class is almost identical to the one you worked with in the **Lecture 18** in-class notebook, so you should look there to remind yourself of the details.   Scroll down to find more information about your tasks as well as unit tests. \n",
    "\n",
    "**Important Note**: In **Problem 2** we'll be using the `Network` class to train a network to do handwritten digit recognition.  Please make sure to utilize vectorized Numpy routines as much as possible, as writing inefficient code here will cause very slow training times in **Problem 2**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-20T03:16:00.042109Z",
     "start_time": "2018-04-20T03:15:59.375181Z"
    },
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Network:\n",
    "    def __init__(self, sizes):\n",
    "        \"\"\"\n",
    "        Initialize the neural network \n",
    "        \n",
    "        :param sizes: a list of the number of neurons in each layer \n",
    "        \"\"\"\n",
    "        # save the number of layers in the network \n",
    "        self.L = len(sizes) \n",
    "        \n",
    "        # store the list of layer sizes \n",
    "        self.sizes = sizes  \n",
    "        \n",
    "        # initialize the bias vectors for each hidden and output layer \n",
    "        self.b = [np.random.randn(n) for n in self.sizes[1:]]\n",
    "        \n",
    "        # initialize the matrices of weights for each hidden and output layer \n",
    "        self.W = [np.random.randn(n, m) for (m,n) in zip(self.sizes[:-1], self.sizes[1:])]\n",
    "        \n",
    "        # initialize the derivatives of biases for backprop \n",
    "        self.db = [np.zeros(n) for n in self.sizes[1:]]\n",
    "        \n",
    "        # initialize the derivatives of weights for backprop \n",
    "        self.dW = [np.zeros((n, m)) for (m,n) in zip(self.sizes[:-1], self.sizes[1:])]\n",
    "        \n",
    "        # initialize the activities on each hidden and output layer \n",
    "        self.z = [np.zeros(n) for n in self.sizes]\n",
    "        \n",
    "        # initialize the activations on each hidden and output layer \n",
    "        self.a = [np.zeros(n) for n in self.sizes]\n",
    "        \n",
    "        # initialize the deltas on each hidden and output layer \n",
    "        self.delta = [np.zeros(n) for n in self.sizes]\n",
    "        \n",
    "        #epoch accuracy dict\n",
    "        self.epoch_accuracy = {}\n",
    "        \n",
    "        # accuracy \n",
    "        self.acc_valid = []\n",
    "        self.acc_train = []\n",
    "        \n",
    "    def g(self, z):\n",
    "        \"\"\"\n",
    "        sigmoid activation function \n",
    "        \n",
    "        :param z: vector of activities to apply activation to \n",
    "        \"\"\"\n",
    "        z = np.clip(z, -20, 20)\n",
    "        return 1.0/(1.0 + np.exp(-z))\n",
    "    \n",
    "    def g_prime(self, z):\n",
    "        \"\"\"\n",
    "        derivative of sigmoid activation function \n",
    "        \n",
    "        :param z: vector of activities to apply derivative of activation to \n",
    "        \"\"\"\n",
    "        return self.g(z) * (1.0 - self.g(z))\n",
    "    \n",
    "    def C(self, a, y):\n",
    "        \"\"\"\n",
    "        evaluate the cost function for squared-loss C(a,y) = ||a-y||^2/2 \n",
    "        \n",
    "        :param a: activations on output layer \n",
    "        :param y: vector-encoded label \n",
    "        \"\"\"\n",
    "        return 0.5 * np.linalg.norm(a - y)**2\n",
    "    \n",
    "    def gradC(self, a, y):\n",
    "        \"\"\"\n",
    "        evaluate gradient of cost function for squared-loss C(a,y) = ||a-y||^2/2 \n",
    "        \n",
    "        :param a: activations on output layer \n",
    "        :param y: vector-encoded label \n",
    "        \"\"\"\n",
    "        return (a - y)\n",
    "    \n",
    "    def forward_prop(self, x):\n",
    "        \"\"\"\n",
    "        take an feature vector and propagate through network \n",
    "        \n",
    "        :param x: input feature vector \n",
    "        \"\"\"\n",
    "        \n",
    "        # TODO: Initialize activation on initial layer to x \n",
    "        self.a[0] = copy.deepcopy(x)\n",
    "        \n",
    "        # TODO: Loop over layers and compute activities and activations \n",
    "        for l in range(self.L - 1):\n",
    "            w = self.W[l]\n",
    "            b = self.b[l]\n",
    "            self.z[l+1] = np.add(np.dot(w, self.a[l]), b)\n",
    "            self.a[l+1] = self.g(self.z[l+1])\n",
    "                \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predicts on the the data in X. Assume at least two output neurons so predictions\n",
    "        are one-hot encoded vectorized labels. \n",
    "        \n",
    "        :param X: a matrix of data to make predictions on \n",
    "        :return y: a matrix of vectorized labels \n",
    "        \"\"\"\n",
    "        yhat = np.zeros((X.shape[0], self.sizes[-1]), dtype=int)\n",
    "        \n",
    "        for x, j in enumerate(X):\n",
    "            self.forward_prop(j)\n",
    "            a = np.argmax(self.a[-1])\n",
    "            yhat[x][a] = 1            \n",
    "                \n",
    "        return yhat \n",
    "    \n",
    "    def accuracy(self, X, y):\n",
    "        \"\"\"\n",
    "        compute accuracy on labeled training set \n",
    "\n",
    "        :param X: matrix of features \n",
    "        :param y: matrix of vectorized true labels \n",
    "        \"\"\"\n",
    "        yhat = self.predict(X)\n",
    "        return np.sum(np.all(np.equal(yhat, y), axis=1)) / X.shape[0]\n",
    "            \n",
    "            \n",
    "    def back_prop(self, x, y):\n",
    "        \"\"\"\n",
    "        Back propagation to get derivatives of C wrt weights and biases for given training example\n",
    "        \n",
    "        :param x: training features  \n",
    "        :param y: vector-encoded label \n",
    "        \"\"\"\n",
    "        \n",
    "        # TODO: forward prop training example to fill in activities and activations\n",
    "        self.forward_prop(x)\n",
    "\n",
    "        # TODO: compute deltas on output layer\n",
    "        delta_c = self.a[-1] - y\n",
    "        de = np.multiply(delta_c, self.g_prime(self.z[-1]))\n",
    "        self.delta[self.L - 1] = de\n",
    "\n",
    "        # TODO: loop backward through layers, backprop deltas, compute dWs and dbs\n",
    "        for ll in range(self.L - 2, -1, -1):\n",
    "            self.dW[ll] = np.multiply(self.delta[ll + 1][:, np.newaxis], self.a[ll][:, np.newaxis].T)\n",
    "            self.db[ll] = self.delta[ll + 1]\n",
    "            temp = np.dot(self.W[ll].T, self.delta[ll + 1])\n",
    "            temp1 = self.g_prime(self.z[ll])\n",
    "            self.delta[ll] = np.multiply(temp, temp1)\n",
    "            \n",
    "        \n",
    "    def gradient_checking(self, X_train, y_train, EPS=0.0001):\n",
    "        \"\"\"\n",
    "        Performs gradient checking on all weights in the \n",
    "        network for a randomly selected training example \n",
    "        :param X_train: matrix of training features \n",
    "        :param y_train: matrix of vector-encoded training labels \n",
    "        \"\"\"\n",
    "        # Randomly select a training example \n",
    "        kk = np.random.randint(0,X_train.shape[0])\n",
    "        xk = X_train[kk]\n",
    "        yk = y_train[kk]\n",
    "\n",
    "        # Get the analytic(ish) weights from back_prop \n",
    "        self.back_prop(xk, yk)\n",
    "\n",
    "        # List of relative errors.  Used only for unit testing. \n",
    "        rel_errors = []\n",
    "\n",
    "        # Loop over and perturb each weight/bias in \n",
    "        # network and test numerical derivative \n",
    "        # Don't forget that after perturbing the weights\n",
    "        # you'll want to put them back the way they were! \n",
    "        \n",
    "        # Loop over and perturb each weight/bias in \n",
    "        # network and test numerical derivative \n",
    "        for ell in range(self.L-1):\n",
    "            for ii in range(self.W[ell].shape[0]):\n",
    "                # Check weights in level W[ell][ii,jj] \n",
    "                for jj in range(self.W[ell].shape[1]):\n",
    "                    \n",
    "                    # TODO true_dW \n",
    "                    true_dW = self.dW[ell][ii,jj]\n",
    "                    # TODO num_dW \n",
    "                    true_W = copy.deepcopy(self.W[ell][ii,jj])\n",
    "                    self.W[ell][ii,jj] = true_W + EPS\n",
    "                    self.forward_prop(xk)\n",
    "                    l1 = self.C(self.a[-1],yk)\n",
    "                    self.W[ell][ii,jj] = true_W - EPS\n",
    "                    self.forward_prop(xk)\n",
    "                    l2 = self.C(self.a[-1],yk)\n",
    "                    num_dW = (l1 - l2) / (2*EPS)\n",
    "                    \n",
    "                    self.W[ell][ii,jj] = true_W\n",
    "                    \n",
    "                    rel_dW = np.abs(true_dW-num_dW)/np.abs(true_dW)\n",
    "                    print(\"W[{:d}][{:d},{:d}]: true: {: 12.10e}  approx: {: 12.10e} rel_err: {: 12.10e}\".format(ell, ii, jj, true_dW, num_dW, rel_dW))\n",
    "                    rel_errors.append(rel_dW)\n",
    "                                    \n",
    "                # TODO num_db  \n",
    "                true_db = self.db[ell][ii]\n",
    "                true_b = copy.deepcopy(self.b[ell][ii])\n",
    "                self.b[ell][ii] = true_b + EPS\n",
    "                self.forward_prop(xk)\n",
    "                l1 = self.C(self.a[-1],yk)\n",
    "                self.b[ell][ii] = true_b - EPS\n",
    "                self.forward_prop(xk)\n",
    "                l2 = self.C(self.a[-1],yk)\n",
    "                num_db = (l1 - l2) / (2*EPS)\n",
    "                self.b[ell][ii] = true_b\n",
    "                \n",
    "                \n",
    "                rel_db = np.abs(true_db-num_db)/np.abs(true_db)\n",
    "                print(\"b[{:d}][{:d}]:   true: {: 12.10e}  approx: {: 12.10e} rel_err: {: 12.10e}\".format(ell, ii, true_db, num_db, rel_db))\n",
    "                rel_errors.append(rel_db)\n",
    "\n",
    "        return rel_errors\n",
    "            \n",
    "            \n",
    "    def train(self, X_train, y_train, X_valid=None, y_valid=None, eta=0.25, lam=0.0, num_epochs=10, isPrint=True):\n",
    "        \"\"\"\n",
    "        Train the network with SGD \n",
    "        \n",
    "        :param X_train: matrix of training features \n",
    "        :param y_train: matrix of vector-encoded training labels \n",
    "        :param X_train: optional matrix of validation features \n",
    "        :param y_train: optional matrix of vector-encoded validation labels \n",
    "        :param eta: learning rate \n",
    "        :param lam: regularization strength \n",
    "        :param num_epochs: number of epochs to run \n",
    "        :param isPrint: flag indicating to print training progress or not \n",
    "        \"\"\"\n",
    "        \n",
    "        import copy\n",
    "        \n",
    "        # initialize shuffled indices \n",
    "        shuffled_inds = list(range(X_train.shape[0]))\n",
    "        \n",
    "        # loop over training epochs \n",
    "        for ep in range(num_epochs):\n",
    "            \n",
    "            # shuffle indices \n",
    "            np.random.shuffle(shuffled_inds)\n",
    "            \n",
    "            # loop over training examples \n",
    "            for ind in shuffled_inds:\n",
    "                \n",
    "                self.back_prop(X_train[ind],y_train[ind]) \n",
    "                dW = copy.deepcopy(self.dW)\n",
    "                sW = copy.deepcopy(self.W)\n",
    "                db = copy.deepcopy(self.db)\n",
    "                self.W = self.W - np.multiply(eta, dW) - np.multiply(sW,lam*eta)\n",
    "                self.b = self.b - np.multiply(eta, db)\n",
    "                \n",
    "            # occasionally print accuracy\n",
    "            if isPrint and ((ep+1)%5)==1:\n",
    "#                 self.epoch_report(ep, num_epochs, X_train, y_train, X_valid, y_valid)\n",
    "                self.epoch_accuracy[ep] = self.accuracy(X_valid, y_valid)\n",
    "                \n",
    "        # print final accuracy\n",
    "        if isPrint:\n",
    "            self.epoch_report(ep, num_epochs, X_train, y_train, X_valid, y_valid)\n",
    "                \n",
    "                    \n",
    "    def epoch_report(self, ep, num_epochs, X_train, y_train, X_valid, y_valid):\n",
    "        \"\"\"\n",
    "        Print the accuracy for the given epoch on training and validation data \n",
    "        \n",
    "        :param ep: the current epoch \n",
    "        :param num_epochs: the total number of epochs\n",
    "        :param X_train: matrix of training features \n",
    "        :param y_train: matrix of vector-encoded training labels \n",
    "        :param X_train: optional matrix of validation features \n",
    "        :param y_train: optional matrix of vector-encoded validation labels \n",
    "        \"\"\"\n",
    "        print(\"epoch {:3d}/{:3d}: \".format(ep+1, num_epochs), end=\"\")\n",
    "        print(\"  train acc: {:8.3f}\".format(self.accuracy(X_train, y_train)), end=\"\")\n",
    "        if X_valid is not None: \n",
    "            print(\"  valid acc: {:8.3f}\".format(self.accuracy(X_valid, y_valid)))\n",
    "            \n",
    "        else: print(\"\")  \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part A**: Complete the `forward_prop` function in the `Network` class to implement forward propagation.  Your function should take in a single training example `x` and propagate it forward in the network, setting the activations and activities on the hidden and output layers.  When you think you're done, execute the following unit test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-20T02:46:58.761543Z",
     "start_time": "2018-04-20T02:46:58.749250Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "testForwardProp (__main__.TestNN) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.005s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbe97c8fef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run -i nn_tests.py \"prob 1A\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part B**: Complete the `predict` function in the `Network` class to take in a matrix of features and return a matrix of one-hot-encoded label predictions. Your one-hot-encoded predictions should correspond to the output neuron with the largest activation.   \n",
    "\n",
    "When you think your `predict` function is working well, execute the following unit test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-20T02:47:01.982781Z",
     "start_time": "2018-04-20T02:47:01.974911Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "testPredict (__main__.TestNN) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.005s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "%run -i nn_tests.py \"prob 1B\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part C**: OK, now it's time to implement back propagation.  Complete the function ``back_prop`` in the ``Network`` class to use a single training example to compute the derivatives of the loss function with respect to the weights and the biases. As in the **Lecture 18** in-class notebook, you may assume that the loss function for a single training example is given by \n",
    "\n",
    "$$\n",
    "C(y, {\\bf a}^L) = \\frac{1}{2}\\|y - {\\bf a}^L\\|^2  \n",
    "$$\n",
    "\n",
    "When you think you're done, execute the following unit test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-20T02:47:04.552132Z",
     "start_time": "2018-04-20T02:47:04.543799Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "testBackProp (__main__.TestNN) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.003s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "%run -i nn_tests.py \"prob 1C\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part D**: Due to the fact that we hard-code our various activation functions, cost functions, and their derivatives, it is vital to do some debugging to make sure we haven't made a mistake.  \n",
    "\n",
    "One common technique is to do **numerical gradient checking**.  In this method we compute numerical approximations of the derivatives of the cost function with respect to the model parameters and compare them to the analytic versions computed by back prop.  \n",
    "\n",
    "Consider a cost function $C$ which is a function of all of the weights and biases in the network.  We can estimate the derivative of $C$ with respect to a particular parameter using a numerical finite difference technique.  This process looks as follows \n",
    "\n",
    "$$\n",
    "\\frac{\\partial C}{\\partial w_i} \\approx \\frac{C(w_1,\\ldots, w_i+\\epsilon, \\ldots w_N) - C(w_1,\\ldots, w_i-\\epsilon, \\ldots w_N)}{2\\epsilon}\n",
    "$$\n",
    "\n",
    "Evaluating the cost function with the perturbed weights can be accomplished by randomly choosing a training example, performing forward propagation, and then evaluating the cost function using the activations in the output layer.  \n",
    "\n",
    "I've given you starter code down below to do numerical gradient checking.  The code will compute the true and numerical values of the derivative of $C$ with respect to each parameter in the network and then plot the pairs of values as well as their relative errors.  Note that in practice this is extremely expensive, and we typically only check a few random parameters. \n",
    "\n",
    "When you believe your code is correct, you can test it by executing the following cell. Note that a good rule of thumb is to train the network for a handful of epochs before doing the gradient checking, to avoid any transient behavior that might occur at the very beginning of the training process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-20T02:47:05.949886Z",
     "start_time": "2018-04-20T02:47:05.941572Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "testGradCheck (__main__.TestNN) ... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W[0][0,0]: true: -1.6129176665e-02  approx: -1.6129176655e-02 rel_err:  6.2758809415e-10\n",
      "W[0][0,1]: true: -3.2258353331e-02  approx: -3.2258353251e-02 rel_err:  2.4817827221e-09\n",
      "b[0][0]:   true: -1.6129176665e-02  approx: -1.6129176655e-02 rel_err:  6.2758809415e-10\n",
      "W[0][1,0]: true:  1.0259238746e-02  approx:  1.0259238739e-02 rel_err:  6.6307893984e-10\n",
      "W[0][1,1]: true:  2.0518477492e-02  approx:  2.0518477439e-02 rel_err:  2.5974560468e-09\n",
      "b[0][1]:   true:  1.0259238746e-02  approx:  1.0259238739e-02 rel_err:  6.6307893984e-10\n",
      "W[0][2,0]: true: -5.1903568015e-04  approx: -5.1903567999e-04 rel_err:  3.0985203531e-10\n",
      "W[0][2,1]: true: -1.0380713603e-03  approx: -1.0380713582e-03 rel_err:  2.0477984656e-09\n",
      "b[0][2]:   true: -5.1903568015e-04  approx: -5.1903567999e-04 rel_err:  3.0985203531e-10\n",
      "W[1][0,0]: true: -7.1372985374e-02  approx: -7.1372985357e-02 rel_err:  2.3383240182e-10\n",
      "W[1][0,1]: true: -4.3055527018e-02  approx: -4.3055527015e-02 rel_err:  8.6212384741e-11\n",
      "W[1][0,2]: true: -7.8283694312e-02  approx: -7.8283694290e-02 rel_err:  2.8537841612e-10\n",
      "b[1][0]:   true: -1.1427975990e-01  approx: -1.1427975983e-01 rel_err:  6.0395790686e-10\n",
      "W[1][1,0]: true:  7.3734308328e-02  approx:  7.3734308308e-02 rel_err:  2.6781078575e-10\n",
      "W[1][1,1]: true:  4.4479987599e-02  approx:  4.4479987595e-02 rel_err:  9.9368482814e-11\n",
      "W[1][1,2]: true:  8.0873653010e-02  approx:  8.0873652984e-02 rel_err:  3.2132963447e-10\n",
      "b[1][1]:   true:  1.1806062206e-01  approx:  1.1806062198e-01 rel_err:  6.8768061143e-10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.010s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "%run -i nn_tests.py \"prob 1D\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part E**: OK, now let's actually train a neural net!  Complete the missing code in ``train`` to loop over the training data in random order, call `back_prop` to get the derivatives, and then update the weights and the biases via SGD.  When you think you're done, execute the following code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-20T02:47:05.949886Z",
     "start_time": "2018-04-20T02:47:05.941572Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "testSGD (__main__.TestNN) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.005s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "%run -i nn_tests.py \"prob 1E\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part F**: Last but not least, we should implement $\\ell$-$2$ regularization.  Modify your `train` function to incorporate regularization of the weights (but **not** the biases) in your SGD update.  As in the Lecture 18 slides, you should assume that the cost function with regularization takes the form \n",
    "\n",
    "$$\n",
    "C_\\lambda = C + \\frac{\\lambda}{2} \\displaystyle\\sum_{w} w^2\n",
    "$$\n",
    "\n",
    "where $\\sum_{w}$ sums over each weight in all layers of the network. Think carefully before you go making large changes to your code.  This modification is much simpler than you think. When you think you're done, execute the following unit test.  (Then go back and execute the test in **Part C** to make sure you didn't break anything.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-20T02:47:07.378983Z",
     "start_time": "2018-04-20T02:47:07.368858Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "testRegularizedSGD (__main__.TestNN) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.006s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "%run -i nn_tests.py \"prob 1F\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [20 points] Problem 2: A Neural Network Classifier for Handwritten Digit Recognition \n",
    "***\n",
    "\n",
    "In this problem you'll use the Feed-Forward Neural Network framework you wrote in **Problem 1** to take an image of a handwritten digit and predict which digit it corresponds to.  \n",
    "\n",
    "![Samples of Handwritten Digits](mnist.png \"MNIST Digits\")\n",
    "\n",
    "To keep run times down we'll again only consider the subset of the MNIST data set consisting of the digits $3, 7, 8$ and $9$. \n",
    "\n",
    "**Part A**: Executing the following cells will load training and validation data and plot an example handwritten digit.  Explore the training and validation sets and answer the following questions: \n",
    "\n",
    "- How many pixels are in each image in the data set?  \n",
    "- How do the true labels correspond to the associated one-hot-encoded label vectors? \n",
    "- Give an example of a network architecture with a single hidden layer that is compatible with this data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-20T03:43:07.591473Z",
     "start_time": "2018-04-20T03:43:07.485465Z"
    },
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train, y_train, X_valid, y_valid = pickle.load(gzip.open(\"../data/mnist21x21_3789_one_hot.pklz\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-20T03:43:09.330860Z",
     "start_time": "2018-04-20T03:43:09.239140Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output vector: [0 1 0 0]\n",
      "Output vector: [0 0 0 1]\n",
      "Output vector: [1 0 0 0]\n",
      "Output vector: [0 0 0 1]\n",
      "Output vector: [0 0 0 1]\n",
      "Output vector: [0 0 1 0]\n",
      "Output vector: [0 1 0 0]\n",
      "Output vector: [0 1 0 0]\n",
      "Output vector: [1 0 0 0]\n",
      "Output vector: [0 0 1 0]\n",
      "Output vector: [0 0 0 1]\n",
      "Output vector: [1 0 0 0]\n",
      "Output vector: [0 0 1 0]\n",
      "Output vector: [0 0 0 1]\n",
      "Output vector: [0 0 1 0]\n",
      "Output vector: [0 0 0 1]\n",
      "Output vector: [0 0 0 1]\n",
      "Output vector: [0 0 1 0]\n",
      "Output vector: [0 1 0 0]\n",
      "Output vector: [0 1 0 0]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAALgAAADHCAYAAACqR5nTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYY3AYY3AYY3AYY3AYe1/y3GaWgxBZ0wAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbe97308320>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAALgAAADHCAYAAACqR5nTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbe97b8e128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAALgAAADHCAYAAACqR5nTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nNAqO0Cg4QqPgCI2CIzQKjtD+DyT5yECHP81jAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbe97c3ef98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAALgAAADHCAYAAACqR5nTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbe97d5c898>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAALgAAADHCAYAAACqR5nTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbe97c83908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAALgAAADHCAYAAACqR5nTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbe98003c50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAALgAAADHCAYAAACqR5nTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAACEJJREFUeJzt3V2IlnkdxvHrp06a+MbMrkKTzmKB\nmO6B4DseVATbwRK5KKzEZlBICbK0BGkHGRSBRxErCFbQgnmS+XqwJoorMWKNYnngSxnrW2iOZiID\njm//DuYxZwef3/3MjDOOl98PzME81/2/73v0mr/3/Of2fqKUIsDVqOd9AsBQouCwRsFhjYLDGgWH\nNQoOaxQc1ig4rFFwWBvTn40jgl97YsQopUTVNszgsEbBYY2CwxoFhzUKDmsUHNYoOKxRcFij4LBG\nwWGNgsMaBYc1Cg5rFBzWKDisUXBYo+CwRsFhjYLDGgWHNQoOaxQc1ig4rFFwWKPgsEbBYY2CwxoF\nhzUKDmsUHNYoOKxRcFij4LBGwWGNgsMaBYc1Cg5r/XqXNTzd2LFj03zcuHFpPmpUPs9Mnz49zVeu\nXFk3W7x4cTq2s7MzzXfu3Jnmhw8fTvObN2+m+VBjBoc1Cg5rFBzWKDisUXBYo+CwRsFhLUopjW8c\n0fjGw2zChAlp3tLSkuZtbW11s/Hjx6dj582bl+ZLly5N8ypV6+CzZs2qm1Wt0T969CjNb9y4keYb\nNmxI823bttXN7t+/n46tUkqJqm2YwWGNgsMaBYc1Cg5rFBzWKDisUXBYGzH3g0+ZMiXNFy1alOZV\na81V45csWVI3mzRpUjr2RTZ69Og0nzZtWpq3trYOeP+DXQdvBDM4rFFwWKPgsEbBYY2CwxoFh7Vh\nXSbMbt1ctWpVOnb9+vVp3tzcnOZVt9MOpe7u7kHlEydOTPOIyrtGB6zq3O7evTtkx34WmMFhjYLD\nGgWHNQoOaxQc1ig4rFFwWBvWdfDs8QvZI4AlacaMGc/6dD7hwYMHdbNbt26lY69evZrme/fuTfMr\nV66k+aZNm9J88uTJdbOqx4JkX7ckbd++Pc13796d5sNxS2yGGRzWKDisUXBYo+CwRsFhjYLDGgWH\ntWFdB8/WXC9dujSofVc9BvjcuXNpfvTo0bpZe3t7OrajoyPNL1++nOZr165N86ampjTPVP25nDhx\nIs23bt2a5ufPn+/3OQ0nZnBYo+CwRsFhjYLDGgWHNQoOaxQc1oZ1Hbyrq6tutnnz5kHtu+rZIQcO\nHEjzHTt21M1u376djq26pzp7mz9JWr58eZpXvY1hpup+7IMHD6b58ePHB3zskYAZHNYoOKxRcFij\n4LBGwWGNgsMaBYe1YV0Hz+5NPnnyZDp23bp1aT5qVP69OpTPuR4zJv9jnDlzZpq3tLQM+NhVOjs7\n03zfvn1pXrXGP9Ixg8MaBYc1Cg5rFBzWKDisUXBYo+CwNqzr4JmHDx+m+Z07d4bpTPqv6j04V6xY\nkeZV6+SDcejQoTQ/c+bMkB17JGAGhzUKDmsUHNYoOKxRcFij4LA2YpYJX2RTp05N89mzZw/p8bPb\nkM+ePZuOfd5v8zfUmMFhjYLDGgWHNQoOaxQc1ig4rFFwWGMdvAFVj4Woejxya2trmpdS0rzq0Q3H\njh2rmx05ciQdyzo48AKj4LBGwWGNgsMaBYc1Cg5rFBzWomoN9hMbRzS+sZHm5uY037NnT5ovW7Zs\nUMe/ePFimq9Zs6ZuVvX2iS+yUkpUbcMMDmsUHNYoOKxRcFij4LBGwWGNgsMa94PXtLW11c02btyY\njl24cOGgjn3v3r00379/f5q3t7cP6vjOmMFhjYLDGgWHNQoOaxQc1ig4rFFwWGMdvGbcuHF1s7lz\n56Zjm5qa0rzquSbXr19P8wsXLqR5V1dXmr/MmMFhjYLDGgWHNQoOaxQc1ig4rLFMWHPt2rW62a5d\nu9Kxc+bMSfNTp06l+datW9O8o6MjzVEfMzisUXBYo+CwRsFhjYLDGgWHNQoOazw+uQFVj0+eP39+\nmmdr7JJ0+vTpNK+63fZlxeOT8dKj4LBGwWGNgsMaBYc1Cg5rFBzW+rsO3ikpf087YHi0lVJerdqo\nXwUHXjRcosAaBYc1Cg5rFLwmIr4eEe897/NoRES8FhEl+Xj7eZ/jSMEPmTUR8VtJXymlfPZ5n0uV\niBgrad5Top9JWibpM6WU/wzvWY1MPDZiACJibCml+3kdv3bsY71fi4jxkhZK2ke5n+ASRf+fvVdL\nau31z/yFWvbF2udvRcSvar8L+PfjcY+367O/jyLioz6vvRIRWyLiXxHRHRFnI2LNM/wy3pI0UdIH\nz3CfLzxm8B4/lfSqpAWSvlZ7re8M/b6kDyW9I6n+w8SfIiImSWqX9GlJP5H0saQ3JG2p/Wvwfq9t\ni6QPSinf6ufXsFrSdUn5m2q+ZCi4pFLKP2sz871SyrE6m/2llPKdAR7iXUltkl4vpfyj9trBiJgi\naWNEbCmlPP5vOw9rHw2LiFZJX5b0y177gbhE6Y/8+W25r0r6s6SPI2LM4w9Jf5TUIukLjzcspYwp\npXy7n/t/Rz1/l1ye9MEM3rirgxg7VdLnJd2vk7cMYt+S9E1Jfy2l/G2Q+7FDwRv3tPXUu5I+9ZTX\nWyTd7PX5TfVcH79bZ9/nBnpSEbFA0mxJ3x/oPpxR8Ce61fNDYH9clDQtIl4ppdyQpIj4nKRZko72\n2m6/pHWSLpVS8nec6r/Vkh5I2v6M92uBa/AnTktqjojvRcSCiHi9gTG/V8/M/ruIeCMiviFpj6Qb\nfbb7hXpm8D9FxHcj4ksR8WZE/CAi9vTeMCIeRMRvGjnhiGiS9LakD4fgG8cCM/gTv5a0WNLPJU1R\nz+z8WjaglHI+Ilao5zeIuyX9XdJ7kn7UZ7vbEbFU0o8l/VBSq6T/qufS5A99dju69tGIN9VzOcQP\nl3Xwq3pY4xIF1ig4rFFwWKPgsEbBYY2CwxoFhzUKDmv/Ayhgy1hJDM7cAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbe97bcd780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAALgAAADHCAYAAACqR5nTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nwWHt/xkur9ze9+24AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbe97c694e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAALgAAADHCAYAAACqR5nTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbe97c34a58>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAALgAAADHCAYAAACqR5nTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nF5b2O4/0pPQxAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbe97cb4da0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAALgAAADHCAYAAACqR5nTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nHNYIHNYIHNYIHNYIHNb+D/DNoy8rdbi6AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbe97d4e898>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAALgAAADHCAYAAACqR5nTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbe97e5f438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAALgAAADHCAYAAACqR5nTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngiO0/wFkfoT/XO56lgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbe97fa8f98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAALgAAADHCAYAAACqR5nTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nETisETisETis/R8+v6R5U2DQnAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbe97e455f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAALgAAADHCAYAAACqR5nTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngsMaBYc1Cg5r/wEbgWgLcMdR3gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbe97cf80b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAALgAAADHCAYAAACqR5nTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAACLtJREFUeJzt3W2IVmkdx/Hf33F0QlDZ1XLcwQlM\n8ylRNNmyZKMXPhDTIGGKZGVDEBKlL8zKstilV9EiK2tQQjHMK9FGFFxdQSETJxQxcVnbNt1gN2d1\nfWZ8nqsX5x6cBs//3PPgztz/+X5gEM/vXPd9oT8ub6/7cI6llARENWKwJwA8SxQcoVFwhEbBERoF\nR2gUHKFRcIRGwREaBUdoI3tzspnxtSeGjJSSFZ3DCo7QKDhCo+AIjYIjNAqO0Cg4QqPgCI2CIzQK\njtAoOEKj4AiNgiM0Co7QKDhCo+AIjYIjNAqO0Cg4QqPgCI2CIzQKjtAoOEKj4AiNgiM0Co7QKDhC\no+AIjYIjNAqO0Hp1d9nhqrq62s0nT57s5suWLXPz2traXs+pu2vXruVmFy5ccMe2tbW5+Y0bN/o0\np6GCFRyhUXCERsERGgVHaBQcoVFwhEbBEdqw2Qc38x/INWnSpNysaB97/fr1bj5nzhw3r6mpcfMi\nDx8+zM3a29vdsdu3b3fzHTt29GlOQwUrOEKj4AiNgiM0Co7QKDhCo+AIjYIjtDD74EX73FOnTnXz\nbdu25WYrVqxwxz548MDNjx8/7ubnz59381GjRrm5t08/bdo0d+yiRYvcvNKxgiM0Co7QKDhCo+AI\njYIjNAqO0Cg4QguzD15fX+/m3j63JK1evTo3K9rH3rVrl5sfPnzYza9evermRXvVK1euzM1u3brl\njt2zZ4+bVzpWcIRGwREaBUdoFByhUXCERsERWsVsE44ZM8bNN2zY4OYNDQ1ufvTo0dxs69at7tjT\np0+7+YgR/joyZcoUN1+7dq2be1ukR44ccceeOHHCzSsdKzhCo+AIjYIjNAqO0Cg4QqPgCI2CI7SK\n2QdfvHixm3uXjErFj8Nrbm7Oze7cueOOnTFjhpsvWbLEzdesWePm8+fPd3Pvkti9e/e6Y2/evOnm\nlY4VHKFRcIRGwREaBUdoFByhUXCERsERWsXsg3d2drr5o0eP3Lyurs7NN2/enJt1dHS4Y4vMmjXL\nzYuudU8puXlra2tutn//fnes9wjCCFjBERoFR2gUHKFRcIRGwREaBUdoFByhWdEe6/+dbFb+yQNs\n7Nixbl50vfiqVavcfPr06blZ0R772bNn3bzoz3jdunVufvfuXTfftGlTbrZ792537OPHj918KEsp\n+c+OFCs4gqPgCI2CIzQKjtAoOEKj4AiNgiO0irkevOhxeEWP6it6FGB1dXVuVrSPPXr0aDffsmWL\nm9fU1Lh5W1ubmx88eDA36+8+98SJE9183Lhxbn7p0qXcrOj7hYHACo7QKDhCo+AIjYIjNAqO0Cg4\nQqPgCK1i9sGLFO333r59+5m998KFC928sbHRzYv2gw8dOuTm/bnH98iRfgWK7l0+b948N9+4cWNu\n9nHcm5wVHKFRcIRGwREaBUdoFByhUXCEFmab8FkqumS0oaHBzevr6938zJkzbl50C+T+qKqqcvPa\n2lo3Hz9+/EBOZ8CxgiM0Co7QKDhCo+AIjYIjNAqO0Cg4QmMfvAwLFixw86ampn69/rlz59y8vb29\nX6/vKXqM4L59+9x87ty5bv5x3BrCwwqO0Cg4QqPgCI2CIzQKjtAoOEKj4AiNffAyzJw5082LbiF8\n/fp1N29tbXXzjo4ON++Pzs5ONz958qSbnzp1ys0H+zGFrOAIjYIjNAqO0Cg4QqPgCI2CIzQKjtDY\nBy+pq6vLzZYuXeqOLboF8YEDB9y8aK95sPeSPYN9vXcRVnCERsERGgVHaBQcoVFwhEbBERoFR2js\ng5d4jwKcPXu2O/by5ctu3tzc7ObP8r4nwx0rOEKj4AiNgiM0Co7QKDhCo+AIzVJK5Z9sVv7JFcbb\nCly+fLk79sqVK27e0tLi5kP9ktOhKqVkReewgiM0Co7QKDhCo+AIjYIjNAqO0Cg4QmMfHBWLfXAM\nexQcoVFwhEbBERoFR2gUHKFRcITW29tGXJX03rOYCNBL9eWc1KsveoBKw0cUhEbBERoFR2gUvMTM\nGs1s02DPo1xmVmVmvzCzi2Z238zeMbMfD/a8hhoK/kSjpIopuKTXJW2VtEvS1yTtlvRbM9s6qLMa\nYri7bB+Y2eiU0v1BfP8pkpokvZxSeqV0+E0zGyvp52b2ekrp2mDNbyhhBZdkZn+S9G1JL5hZKv1c\nKmUvlX6/0sz+YGZXJLV3jes6r8frHTOzYz2OTTCznWb2fukjxdtm9v0+TnmRsr+7gz2OvyGpRpJ/\nI5dhhBU887KkiZI+L6mhdKznCv2askJ9S1mJylZaWf8m6ROSfiXpoqSlknaW/jV4rdu5SdKfU0rf\ncV6y68GZD3oc75rznN7MLzIKLiml9G5pZX6QUsp7KuvfU0pNfXyLHyn75u1zKaV3SseOmNl4SdvM\nbGdKqev2Vo/1pMB5LpR+fVHSmW7Hv1D69bk+zjMcCl6+v/Rj7DJJbZIumln3P/NDyj5Lz5L0D0lK\nKRX+naSU3jKzNyX92sz+XXrtr0jq2kXp7MdcQ6Hg5ftvP8Z+UtJnJD3MyZ/vw2t+V1KLss/dknRL\n0mZJv1f/5hoKBS/f0y7auSdp1FOOPy/po26//0jSh8o+qjzNhZzj+ZNJ6X1JL5nZZGUfSd6VNLcU\nH+/t60VFwZ+4r+w/gb3xnqRPmdmElNJVSTKzqZI+K+lEt/PekPRDSf9JKX04EJPtklL6QNIHZmbK\nPqK8LenYQL5HJaPgT7wl6Tkz+4GkU5LupZTOFYzZrWwHpsXMfidpgqSfKrusuLtXJX1T0l/N7FVl\nK/YYSTMkfTml9PWuE83skbJdlO95b1ya5z1lOzKTlG1zfknSV1NKfAYvoeBP/FHZrsRvJI1Xtjp/\n2huQUvqXmX1D0iuSWiX9U9m3oT/rcd5NM/uipF9K+omkFyTdUFb0PT1etqr0U6RK0hZluzMdylbt\nF1NK58sYO2xwPThC45tMhEbBERoFR2gUHKFRcIRGwREaBUdoFByh/Q/sQ+F7uPVnjQAAAABJRU5E\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbe97d0bc88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAALgAAADHCAYAAACqR5nTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbe97dc6828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAALgAAADHCAYAAACqR5nTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAACRBJREFUeJzt3W2IlWkdx/Hf39EZSx1t1RINEhzK\n8llwBUFxMTJpylHIF0oklumKsSktIqwa2buEfaGkKeqKhKyBq1KsLdEOPpEFA4aUJeqsOJjrU+b4\nMOM4Vy/OEWeHvf/3jLPbHP/n+4GDzPnd1zmX489rzlxzz30spSQgqj69PQHg00TBERoFR2gUHKFR\ncIRGwREaBUdoFByhUXCE1rc7B5sZP/ZEyUgpWd4xrOAIjYIjNAqO0Cg4QqPgCI2CIzQKjtAoOEKj\n4AiNgiM0Co7QKDhCo+AIjYIjNAqO0Cg4QqPgCI2CIzQKjtAoOEKj4AitW79VX66qqqrcfPjw4W4+\nYsQINx8yZIibm/m/PN7e3p6Z3bp1yx178+ZNN29qanLzUr++PCs4QqPgCI2CIzQKjtAoOEKj4AiN\ngiO0stkHz9tLnjhxYmZWW1vrjp0+fbqbT5o0yc1HjRrl5hUVFW7e1taWmV28eNEde+bMGTfftGmT\nmzc2Nrp5b2MFR2gUHKFRcIRGwREaBUdoFByhUXCEFmYfvH///m6+aNEiN1++fHlmlreP/fjxYze/\ndOmSm586dcrN8/ayx44dm5nNmTPHHTtlyhQ3Hzp0qJuzDw70IgqO0Cg4QqPgCI2CIzQKjtAoOEIL\nsw8+cOBAN1+zZo2bjxs3LjM7ffq0O3bHjh1ufvz4cTdvbm5288rKSjdfvXp1ZlZTU+OOPXDggJuf\nP3/ezUsdKzhCo+AIjYIjNAqO0Cg4QqPgCI2CI7Qw++B516lubW197sc+ceKEmx85csTNHz586OZ5\n51zX1dW5+fz58zOzbdu2uWP379/v5j35vJUCVnCERsERGgVHaBQcoVFwhEbBEVqYbcL79++7+cGD\nB918woQJmVneJSfyLuuQ9zaCM2bMcPPJkye7ufc2gufOnXPHvujbgHlYwREaBUdoFByhUXCERsER\nGgVHaBQcoVneaaYfOdis6weXmJEjR7r5hg0bMrPFixf36Ln79vV/3JC3j97Q0ODmJ0+ezMwOHTrk\njr19+7abl7KUkv/ekGIFR3AUHKFRcIRGwREaBUdoFByhUXCEVjb74HlmzZqVme3evdsdO2bMmB49\n99q1a918586dbv7gwYMePf+Lin1wlD0KjtAoOEKj4AiNgiM0Co7QKDhCC3NdlDzV1dVuXltbm5nl\nXd4473zuvHPRly1b5ubXrl1z88OHD2dmLS0t7tjoWMERGgVHaBQcoVFwhEbBERoFR2gUHKGVzfng\n48ePd/N9+/ZlZlevXnXHbt682c0XLFjg5kuXLnXzR48eufnWrVszs71797pj79696+aljPPBUfYo\nOEKj4AiNgiM0Co7QKDhCo+AILcz54P369XPzadOmufnYsWMzs127drljz5496+YXLlxw8xs3brj5\nunXr3Hz9+vWZWd4evncuuSS1tbW5ealjBUdoFByhUXCERsERGgVHaBQcoYXZJqysrHTzqVOnurlZ\n9pmX169fd8e2t7e7ed4pqXv27HHzAQMGuPnGjRszs7lz57pjjx075ubNzc1uXupYwREaBUdoFByh\nUXCERsERGgVHaBQcoYXZB3/y5ImbNzU1ubl3meERI0a4YysqKtw8b255l4W4dOmSm9+7dy8zGzRo\nkDu2T5/Ya1zsvx3KHgVHaBQcoVFwhEbBERoFR2gUHKGF2QfPe7u8o0ePuvnChQszs5UrV7pj79y5\n4+Z554uPHj3azZcsWeLm3vniDQ0N7tjW1lY3f9GxgiM0Co7QKDhCo+AIjYIjNAqO0Cg4QiubtxEc\nPHiwm69atSozW7FiRY+eu3///m5eVVXl5nnXJvEugbxlyxZ37JUrV9y8O/34f+NtBFH2KDhCo+AI\njYIjNAqO0Cg4QqPgCK1s9sG9639LUnV1dWY2c+ZMd+y8efPcvKamxs0bGxvdPO8a3vX19ZlZ3rXJ\n885VL2Xsg6PsUXCERsERGgVHaBQcoVFwhFY224SIh21ClD0KjtAoOEKj4AiNgiM0Co7QKDhCo+AI\njYIjNAqO0Cg4QqPgCI2CIzQKjtAoOELr7tsI3pT0wacxEaCbvtSVg7r1Cw/Ai4aXKAiNgiM0Co7Q\nKHiRmdWZ2drenkdXmVmFma0xs3Nmdt/MrpnZO2Y2sbfnVkoo+DN1kl6YgkvaLGmLpMOSvi3pNUlj\nJL1vZl/szYmVku5uE0KSmVWllFp6eRpLJb2dUnrj6R1m9jdJ/5D0LUm/7qV5lRRWcElm9pak70sa\nZWapeGssZrOLHy80s11mdkPS9afjnh7X6fHqzay+033DzGy7mTWZWYuZnTezH/Vg2pWS/tvpvv8U\n/+TftYgVvGCzpOGSpkn6TvG+ziv0VknvSvqeJP+NLzsxs2pJpyR9RtLPJF2WNFfS9uJXg60djk2S\n9qWUluY87K8krTWzdyW9L2mYCi9Zrkp6uzvzi4yCS0opXSyuzK0ppT9nHPaXlNIPn/MpXlPhJ28T\nUkoXivf90cyGSNpkZttTSm3F+58Ub3lz3mhmLZIO6dmK/S9Js1NKt59znuHwpazr3unB2G9KOiPp\nspn1fXqT9AdJQyV97emBKaW+KaUf5D2gmb0q6Q1Jv5D0iqTvSron6T0zG9mDuYbCCt5113ow9vOS\naiQ9zsiHdufBzOwlSW9K+mVKaVOH+/8kqVHS65LWPNdMg6HgXfdxJ+08UuGbvc6GSrrV4eNbkj5U\n4aXKx/lnN+fyZUlVkv76kQmmdNvMLkr6ajcfLywK/kyLCt8EdscHkr5gZsNSSjclyczGSPqKpNMd\njjsm6ceSrqSUPvwE5vrv4p8vSzr69M7iyl4jqeETeI4QeA3+zN8lvWRmr5rZNDOb0IUxv1VhZf+N\nmc01syWSjqhwWnFHb6qwgp8ws5Vm9oqZ1ZrZT83sSMcDzazNzHZ7T5pSapT0O0mvm9nPzWyOmS2S\n9J4KK/v2Lsy9PKSUuBVOGR4g6YCkOyqUtrF4/+zix1/PGFcn6Zykh5LOSvqGpHpJ9Z2O+5wKRb8s\nqVXFwkv6SafjkqS3ujDfz0raoMJ/zPsqfI/we0kv9/bnspRunA+O0HiJgtAoOEKj4AiNgiM0Co7Q\nKDhCo+AIjYIjtP8BzQzLeSyzNwwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbe97e9ec88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAALgAAADHCAYAAACqR5nTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFBzWKDis/Q9WZrcVcaEifAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbe972c97b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAALgAAADHCAYAAACqR5nTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\njYLDGgWHtf8BTluizoeRTUIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbe97dfd438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def view_digit(x, label=None):\n",
    "    fig = plt.figure(figsize=(3,3))\n",
    "    plt.imshow(x.reshape(21,21), cmap='gray');\n",
    "    plt.xticks([]); plt.yticks([]);\n",
    "    if label: plt.xlabel(\"true: {}\".format(label), fontsize=16)\n",
    "        \n",
    "training_index = 10\n",
    "for training_index in range(20):\n",
    "    label_dict = dict({0:3, 1:7, 2:8, 3:9, 4:4, 5:5, 6:6})\n",
    "    view_digit(X_train[training_index], label_dict[np.argmax(y_train[training_index])])\n",
    "    print(\"Output vector: \" + str(y_train[training_index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(\"1. Number of pixels in each training example are \" + str(X_train[0].shape))\n",
    "print(\"2. One hot encoding is nothing but binary representation of digit3 3,7,8,9 \\n e.g.\" + str(y_train[2]))\n",
    "print(\"3. The features are pixels here. There are 441 features. According to our network\"+\n",
    "     \" the input neurons are number of features. Hence the input layer should be 441 neurons. \\n\"+\n",
    "     \"Next, there are n neurons in hidden layer. \\n\"+\n",
    "     \"The output layer consists of the output vector y. We are considering for 0 - 9 digits, \"+\n",
    "     \"hence, the number of output layer neurons is 4.\\n\"+\n",
    "     \"The output architecture should look like following \\n\"+\n",
    "     \"[441, n, 4]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Number of pixels in each training example are (441,)\n",
      "2. One hot encoding is nothing but binary representation of digit3 3,7,8,9 \n",
      " e.g.[1 0 0 0]\n",
      " 1st bit is for 3, 2nd bit is for 7, 3rd for 8 and 4th for 9.\n",
      "3. The features are pixels here. There are 441 features. According to our network the input neurons are number of features. Hence the input layer should be 441 neurons. \n",
      "Next, there are n neurons in hidden layer. \n",
      "The output layer consists of the output vector y. We are considering for 0 - 9 digits, hence, the number of output layer neurons is 4.\n",
      "The output architecture should look like following \n",
      "[441, n, 4]\n"
     ]
    }
   ],
   "source": [
    "print(\"1. Number of pixels in each training example are \" + str(X_train[0].shape))\n",
    "print(\"2. One hot encoding is nothing but binary representation of digit3 3,7,8,9 \\n e.g.\" + str(y_train[2])+\n",
    "     \"\\n 1st bit is for 3, 2nd bit is for 7, 3rd for 8 and 4th for 9.\")\n",
    "print(\"3. The features are pixels here. There are 441 features. According to our network\"+\n",
    "     \" the input neurons are number of features. Hence the input layer should be 441 neurons. \\n\"+\n",
    "     \"Next, there are n neurons in hidden layer. \\n\"+\n",
    "     \"The output layer consists of the output vector y. We are considering for 0 - 9 digits, \"+\n",
    "     \"hence, the number of output layer neurons is 4.\\n\"+\n",
    "     \"The output architecture should look like following \\n\"+\n",
    "     \"[441, n, 4]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part B**: Train a network with a single hidden layer containing $30$ neurons on the first $500$ training examples in the training set using a learning rate of $\\eta = 0.01$ for at least $50$ epochs.  What accuracy does your network achieve on the validation set?  Do you see any clear signs of overfitting?   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch   1/ 50:   train acc:    0.268\n",
      "epoch  11/ 50:   train acc:    0.570\n",
      "epoch  21/ 50:   train acc:    0.724\n",
      "epoch  31/ 50:   train acc:    0.806\n",
      "epoch  41/ 50:   train acc:    0.834\n",
      "epoch  50/ 50:   train acc:    0.858\n"
     ]
    }
   ],
   "source": [
    "nn = Network([441,30,4])\n",
    "nn.train(X_train[0:500],y_train[0:500], eta=0.01, lam=0.0, num_epochs=50, isPrint=True)\n",
    "\"\"\"\n",
    "Overfitting occurs when the training accuracy increases with number of training examples. Here you can clearly see\n",
    "that the accuracy is increasing with training examples. While the validation\n",
    "accuracy remain 80%, training accuracy increases upto 85%. Neural network is trying to overfit the data.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.809"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.accuracy(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part C**: Modify the `Network` class so that it stores the accuracies on the training and validation data every $5$ epochs during the training process. Now increase the number of neurons in the hidden layer to $100$.  On a single set of axes, plot the **validation accuracy** vs epoch for networks trained on the full training set for at least 50 epochs using the learning rates $\\eta = 0.01$, $\\eta = 0.25$ and $\\eta = 1.5$.  Which learning rate seems to perform the best? What is the best accuracy achieved on the validation set?   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  50/ 50:   train acc:    0.720  valid acc:    0.702\n",
      "epoch  50/ 50:   train acc:    0.997  valid acc:    0.950\n",
      "epoch  50/ 50:   train acc:    0.991  valid acc:    0.957\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbe97dcb5c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eta = [0.01, 0.25, 1.5]\n",
    "for e in eta:\n",
    "    nn = Network([441,100,4])\n",
    "    nn.train(X_train, y_train, X_valid=X_valid, y_valid=y_valid, eta=e, lam=0.0, num_epochs=50, isPrint=True)\n",
    "    x, y = nn.epoch_accuracy.keys(), nn.epoch_accuracy.values()\n",
    "    plt.plot(x,y, label=\"eta = %s\" %e)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "As seen from the graph the learning accuracy increases with increasing eta. \n",
    "The best solution is seen with eta = 1.5\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part D**:  Now let's see if we can get better results with regularization. Using the best learning rate you found in **Part C**, on a single set of axes, plot the **validation accuracy** vs epoch for networks trained on the full training set for at least 50 epochs using the regularization strengths $\\lambda = 10^{-6}$, $\\lambda = 10^{-4}$ and $\\lambda = 10^{-2}$.  Which regularization strength seems to perform the best? What is the best accuracy achieved on the validation set?   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  50/ 50:   train acc:    0.997  valid acc:    0.961\n",
      "epoch  50/ 50:   train acc:    0.964  valid acc:    0.950\n",
      "epoch  50/ 50:   train acc:    0.519  valid acc:    0.529\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n+ZLE5s1DDQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0e088fc978>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'\\nAccording to the graph below, the lam = 10^-6 is better because as lambda is increasing the sgd is diverging.\\nHence it is not finding the local minima properly\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lamda = [10**(-6), 10**(-4), 10**(-2)]\n",
    "for j in lamda:\n",
    "    nn = Network([441,100,4])\n",
    "    nn.train(X_train, y_train, X_valid=X_valid, y_valid=y_valid, eta=1.5, lam=j, num_epochs=50, isPrint=True)\n",
    "    x, y = nn.epoch_accuracy.keys(), nn.epoch_accuracy.values()\n",
    "    plt.plot(x,y, label='lam = %s' %j)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\"\"\"\n",
    "According to the graph below, the lam = 10^-6 is better because as lambda is increasing the sgd is diverging.\n",
    "Hence it is not finding the local minima. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part E**:  Now let's see if we can get better results with different network architectures. On a single set of axes, plot the **validation accuracy** vs epoch for networks trained on the full training set for at least 50 epochs using the architecture from **Part D** as well as two other architectures.  Which architecture seems to perform the best? What is the best accuracy achieved on the validation set?   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  50/ 50:   train acc:    0.991  valid acc:    0.965\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEICAYAAACzliQjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbe97c53518>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  50/ 50:   train acc:    0.995  valid acc:    0.969\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEICAYAAACzliQjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbe97cc3908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  50/ 50:   train acc:    0.992  valid acc:    0.972\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEICAYAAACzliQjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nD1PR45Bj/tR5AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbe97d947f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Trying out the other network architectures with different number of hidden neurons with best eta and lambda values\n",
    "\"\"\"\n",
    "nn = Network([441,100,4])\n",
    "lamda = [10**(-6), 10**(-4), 10**(-2)]\n",
    "nn.train(X_train, y_train, X_valid=X_valid, y_valid=y_valid, eta=1.5, lam=10**(-6), num_epochs=50, isPrint=True)\n",
    "x, y = nn.epoch_accuracy.keys(), nn.epoch_accuracy.values()\n",
    "plt.title(\"Hidden layer size = 100\")\n",
    "plt.plot(x,y)\n",
    "plt.show()\n",
    "\n",
    "nn = Network([441,150,4])\n",
    "lamda = [10**(-6), 10**(-4), 10**(-2)]\n",
    "nn.train(X_train, y_train, X_valid=X_valid, y_valid=y_valid, eta=1.5, lam=10**(-6), num_epochs=50, isPrint=True)\n",
    "x, y = nn.epoch_accuracy.keys(), nn.epoch_accuracy.values()\n",
    "plt.title(\"Hidden layer size = 150\")\n",
    "plt.plot(x,y)\n",
    "plt.show()\n",
    "\n",
    "nn = Network([441,120,4])\n",
    "lamda = [10**(-6), 10**(-4), 10**(-2)]\n",
    "nn.train(X_train, y_train, X_valid=X_valid, y_valid=y_valid, eta=1.5, lam=10**(-6), num_epochs=50, isPrint=True)\n",
    "x, y = nn.epoch_accuracy.keys(), nn.epoch_accuracy.values()\n",
    "plt.title(\"Hidden layer size = 120\")\n",
    "plt.plot(x,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Among all the above architectures, the best accuracy achieved is on 3rd when number of neurons in hidden layer is 150. \n",
    "Which makes sense, because the more features you extract the better your prediction becomes. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [max 20 points] Extra Credit: Improving Network Performance \n",
    "***\n",
    "\n",
    "See if you can get better performance by exploring advanced techniques.  Things you might try are: \n",
    "\n",
    "- Implementing **Mini-Batch** Stochastic Gradient Descent \n",
    "- Experimenting with different activation functions (like tanh and **Leaky** ReLU)\n",
    "- Experimenting with different loss functions (like cross-entropy) \n",
    "\n",
    "For more detailed discussion of these techniques it'll be helpful to look at Chapter 3 of [Nielsen](http://neuralnetworksanddeeplearning.com/chap3.html). \n",
    "\n",
    "The amount of extra credit you receive will be proportional to the number of above suggested tasks that you complete.  Further, to receive credit for the tasks you must not only implement, but also provide evidence that you've tuned the network to make it work.  Comment on the performance differences between the original `Network` implementation and your new networks with bells and whistles. \n",
    "\n",
    "**Important Note**: Don't do any of these things in the original `Network` class, because you'll almost certainly break the unit tests.  Copy the `Network` class from above and rename it `BetterNetwork` (or something) and modify the new class. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Code for mini batch GD\n",
    "\"\"\"\n",
    "from numpy import random\n",
    "class Network_mini:\n",
    "    def __init__(self, sizes):\n",
    "        \"\"\"\n",
    "        Initialize the neural network \n",
    "        \n",
    "        :param sizes: a list of the number of neurons in each layer \n",
    "        \"\"\"\n",
    "        # save the number of layers in the network \n",
    "        self.L = len(sizes) \n",
    "        \n",
    "        # store the list of layer sizes \n",
    "        self.sizes = sizes  \n",
    "        \n",
    "        # initialize the bias vectors for each hidden and output layer \n",
    "        self.b = [np.random.randn(n) for n in self.sizes[1:]]\n",
    "        \n",
    "        # initialize the matrices of weights for each hidden and output layer \n",
    "        self.W = [np.random.randn(n, m) for (m,n) in zip(self.sizes[:-1], self.sizes[1:])]\n",
    "        \n",
    "        # initialize the derivatives of biases for backprop \n",
    "        self.db = [np.zeros(n) for n in self.sizes[1:]]\n",
    "        \n",
    "        # initialize the derivatives of weights for backprop \n",
    "        self.dW = [np.zeros((n, m)) for (m,n) in zip(self.sizes[:-1], self.sizes[1:])]\n",
    "        \n",
    "        # initialize the activities on each hidden and output layer \n",
    "        self.z = [np.zeros(n) for n in self.sizes]\n",
    "        \n",
    "        # initialize the activations on each hidden and output layer \n",
    "        self.a = [np.zeros(n) for n in self.sizes]\n",
    "        \n",
    "        # initialize the deltas on each hidden and output layer \n",
    "        self.delta = [np.zeros(n) for n in self.sizes]\n",
    "        \n",
    "        #epoch accuracy dict\n",
    "        self.epoch_accuracy = {}\n",
    "        \n",
    "        # accuracy \n",
    "        self.acc_valid = []\n",
    "        self.acc_train = []\n",
    "        \n",
    "    def g(self, z):\n",
    "        \"\"\"\n",
    "        sigmoid activation function \n",
    "        \n",
    "        :param z: vector of activities to apply activation to \n",
    "        \"\"\"\n",
    "        z = np.clip(z, -20, 20)\n",
    "        return 1.0/(1.0 + np.exp(-z))\n",
    "    \n",
    "    def g_prime(self, z):\n",
    "        \"\"\"\n",
    "        derivative of sigmoid activation function \n",
    "        \n",
    "        :param z: vector of activities to apply derivative of activation to \n",
    "        \"\"\"\n",
    "        return self.g(z) * (1.0 - self.g(z))\n",
    "    \n",
    "    def C(self, a, y):\n",
    "        \"\"\"\n",
    "        evaluate the cost function for squared-loss C(a,y) = ||a-y||^2/2 \n",
    "        \n",
    "        :param a: activations on output layer \n",
    "        :param y: vector-encoded label \n",
    "        \"\"\"\n",
    "        return 0.5 * np.linalg.norm(a - y)**2\n",
    "    \n",
    "    def gradC(self, a, y):\n",
    "        \"\"\"\n",
    "        evaluate gradient of cost function for squared-loss C(a,y) = ||a-y||^2/2 \n",
    "        \n",
    "        :param a: activations on output layer \n",
    "        :param y: vector-encoded label \n",
    "        \"\"\"\n",
    "        return (a - y)\n",
    "    \n",
    "    def forward_prop(self, x):\n",
    "        \"\"\"\n",
    "        take an feature vector and propagate through network \n",
    "        \n",
    "        :param x: input feature vector \n",
    "        \"\"\"\n",
    "        \n",
    "        # TODO: Initialize activation on initial layer to x \n",
    "        self.a[0] = copy.deepcopy(x)\n",
    "        \n",
    "        # TODO: Loop over layers and compute activities and activations \n",
    "        for l in range(self.L - 1):\n",
    "            w = self.W[l]\n",
    "            b = self.b[l]\n",
    "            self.z[l+1] = np.add(np.dot(w, self.a[l]), b)\n",
    "            self.a[l+1] = self.g(self.z[l+1])\n",
    "                \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predicts on the the data in X. Assume at least two output neurons so predictions\n",
    "        are one-hot encoded vectorized labels. \n",
    "        \n",
    "        :param X: a matrix of data to make predictions on \n",
    "        :return y: a matrix of vectorized labels \n",
    "        \"\"\"\n",
    "        yhat = np.zeros((X.shape[0], self.sizes[-1]), dtype=int)\n",
    "        \n",
    "        for x, j in enumerate(X):\n",
    "            self.forward_prop(j)\n",
    "            a = np.argmax(self.a[-1])\n",
    "            yhat[x][a] = 1            \n",
    "                \n",
    "        return yhat \n",
    "    \n",
    "    def accuracy(self, X, y):\n",
    "        \"\"\"\n",
    "        compute accuracy on labeled training set \n",
    "\n",
    "        :param X: matrix of features \n",
    "        :param y: matrix of vectorized true labels \n",
    "        \"\"\"\n",
    "        yhat = self.predict(X)\n",
    "        return np.sum(np.all(np.equal(yhat, y), axis=1)) / X.shape[0]\n",
    "            \n",
    "            \n",
    "    def back_prop(self, x, y):\n",
    "        \"\"\"\n",
    "        Back propagation to get derivatives of C wrt weights and biases for given training example\n",
    "        \n",
    "        :param x: training features  \n",
    "        :param y: vector-encoded label \n",
    "        \"\"\"\n",
    "        \n",
    "        # TODO: forward prop training example to fill in activities and activations\n",
    "        self.forward_prop(x)\n",
    "\n",
    "        # TODO: compute deltas on output layer\n",
    "        delta_c = self.a[-1] - y\n",
    "        de = np.multiply(delta_c, self.g_prime(self.z[-1]))\n",
    "        self.delta[self.L - 1] = de\n",
    "\n",
    "        # TODO: loop backward through layers, backprop deltas, compute dWs and dbs\n",
    "        for ll in range(self.L - 2, -1, -1):\n",
    "            self.dW[ll] = np.multiply(self.delta[ll + 1][:, np.newaxis], self.a[ll][:, np.newaxis].T)\n",
    "            self.db[ll] = self.delta[ll + 1]\n",
    "            temp = np.dot(self.W[ll].T, self.delta[ll + 1])\n",
    "            temp1 = self.g_prime(self.z[ll])\n",
    "            self.delta[ll] = np.multiply(temp, temp1)\n",
    "            \n",
    "        \n",
    "    def gradient_checking(self, X_train, y_train, EPS=0.0001):\n",
    "        \"\"\"\n",
    "        Performs gradient checking on all weights in the \n",
    "        network for a randomly selected training example \n",
    "        :param X_train: matrix of training features \n",
    "        :param y_train: matrix of vector-encoded training labels \n",
    "        \"\"\"\n",
    "        # Randomly select a training example \n",
    "        kk = np.random.randint(0,X_train.shape[0])\n",
    "        xk = X_train[kk]\n",
    "        yk = y_train[kk]\n",
    "\n",
    "        # Get the analytic(ish) weights from back_prop \n",
    "        self.back_prop(xk, yk)\n",
    "\n",
    "        # List of relative errors.  Used only for unit testing. \n",
    "        rel_errors = []\n",
    "\n",
    "        # Loop over and perturb each weight/bias in \n",
    "        # network and test numerical derivative \n",
    "        # Don't forget that after perturbing the weights\n",
    "        # you'll want to put them back the way they were! \n",
    "        \n",
    "        # Loop over and perturb each weight/bias in \n",
    "        # network and test numerical derivative \n",
    "        for ell in range(self.L-1):\n",
    "            for ii in range(self.W[ell].shape[0]):\n",
    "                # Check weights in level W[ell][ii,jj] \n",
    "                for jj in range(self.W[ell].shape[1]):\n",
    "                    \n",
    "                    # TODO true_dW \n",
    "                    true_dW = self.dW[ell][ii,jj]\n",
    "                    # TODO num_dW \n",
    "                    true_W = copy.deepcopy(self.W[ell][ii,jj])\n",
    "                    self.W[ell][ii,jj] = true_W + EPS\n",
    "                    self.forward_prop(xk)\n",
    "                    l1 = self.C(self.a[-1],yk)\n",
    "                    self.W[ell][ii,jj] = true_W - EPS\n",
    "                    self.forward_prop(xk)\n",
    "                    l2 = self.C(self.a[-1],yk)\n",
    "                    num_dW = (l1 - l2) / (2*EPS)\n",
    "                    \n",
    "                    self.W[ell][ii,jj] = true_W\n",
    "                    \n",
    "                    rel_dW = np.abs(true_dW-num_dW)/np.abs(true_dW)\n",
    "                    print(\"W[{:d}][{:d},{:d}]: true: {: 12.10e}  approx: {: 12.10e} rel_err: {: 12.10e}\".format(ell, ii, jj, true_dW, num_dW, rel_dW))\n",
    "                    rel_errors.append(rel_dW)\n",
    "                                    \n",
    "                # TODO num_db  \n",
    "                true_db = self.db[ell][ii]\n",
    "                true_b = copy.deepcopy(self.b[ell][ii])\n",
    "                self.b[ell][ii] = true_b + EPS\n",
    "                self.forward_prop(xk)\n",
    "                l1 = self.C(self.a[-1],yk)\n",
    "                self.b[ell][ii] = true_b - EPS\n",
    "                self.forward_prop(xk)\n",
    "                l2 = self.C(self.a[-1],yk)\n",
    "                num_db = (l1 - l2) / (2*EPS)\n",
    "                self.b[ell][ii] = true_b\n",
    "                \n",
    "                \n",
    "                rel_db = np.abs(true_db-num_db)/np.abs(true_db)\n",
    "                print(\"b[{:d}][{:d}]:   true: {: 12.10e}  approx: {: 12.10e} rel_err: {: 12.10e}\".format(ell, ii, true_db, num_db, rel_db))\n",
    "                rel_errors.append(rel_db)\n",
    "\n",
    "        return rel_errors\n",
    "            \n",
    "            \n",
    "    def train(self, X_train, y_train, X_valid=None, y_valid=None, eta=0.25, lam=0.0, num_epochs=10, isPrint=True):\n",
    "        \"\"\"\n",
    "        Train the network with SGD \n",
    "        \n",
    "        :param X_train: matrix of training features \n",
    "        :param y_train: matrix of vector-encoded training labels \n",
    "        :param X_train: optional matrix of validation features \n",
    "        :param y_train: optional matrix of vector-encoded validation labels \n",
    "        :param eta: learning rate \n",
    "        :param lam: regularization strength \n",
    "        :param num_epochs: number of epochs to run \n",
    "        :param isPrint: flag indicating to print training progress or not \n",
    "        \"\"\"\n",
    "        \n",
    "        import copy\n",
    "        \n",
    "        # initialize shuffled indices \n",
    "        shuffled_inds = list(range(X_train.shape[0]))\n",
    "        \n",
    "        # loop over training epochs \n",
    "        for ep in range(num_epochs):\n",
    "            \n",
    "            # shuffle indices \n",
    "            np.random.shuffle(shuffled_inds)\n",
    "            \n",
    "            # loop over training examples \n",
    "            for ind in shuffled_inds:\n",
    "                \n",
    "                self.back_prop(X_train[ind],y_train[ind]) \n",
    "                dW = copy.deepcopy(self.dW)\n",
    "                sW = copy.deepcopy(self.W)\n",
    "                db = copy.deepcopy(self.db)\n",
    "                self.W = self.W - np.multiply(eta, dW) - np.multiply(sW,lam*eta)\n",
    "                self.b = self.b - np.multiply(eta, db)\n",
    "                \n",
    "            # occasionally print accuracy\n",
    "            if isPrint and ((ep+1)%5)==1:\n",
    "#                 self.epoch_report(ep, num_epochs, X_train, y_train, X_valid, y_valid)\n",
    "                self.epoch_accuracy[ep] = self.accuracy(X_valid, y_valid)\n",
    "                \n",
    "        # print final accuracy\n",
    "        if isPrint:\n",
    "            self.epoch_report(ep, num_epochs, X_train, y_train, X_valid, y_valid)\n",
    "                \n",
    "                    \n",
    "    def epoch_report(self, ep, num_epochs, X_train, y_train, X_valid, y_valid):\n",
    "        \"\"\"\n",
    "        Print the accuracy for the given epoch on training and validation data \n",
    "        \n",
    "        :param ep: the current epoch \n",
    "        :param num_epochs: the total number of epochs\n",
    "        :param X_train: matrix of training features \n",
    "        :param y_train: matrix of vector-encoded training labels \n",
    "        :param X_train: optional matrix of validation features \n",
    "        :param y_train: optional matrix of vector-encoded validation labels \n",
    "        \"\"\"\n",
    "        print(\"epoch {:3d}/{:3d}: \".format(ep+1, num_epochs), end=\"\")\n",
    "        print(\"  train acc: {:8.3f}\".format(self.accuracy(X_train, y_train)), end=\"\")\n",
    "        if X_valid is not None: \n",
    "            print(\"  valid acc: {:8.3f}\".format(self.accuracy(X_valid, y_valid)))\n",
    "            \n",
    "        else: print(\"\")  \n",
    "            \n",
    "    def get_mini_batches(self, X, y, batch_size):\n",
    "        random_idxs = random.choice(len(y), len(y), replace=False)\n",
    "        X_shuffled = X[random_idxs, :]\n",
    "        y_shuffled = y[random_idxs]\n",
    "        mini_batches = [(X_shuffled[i:i + batch_size, :], y_shuffled[i:i + batch_size]) for\n",
    "                        i in range(0, len(y), batch_size)]\n",
    "        return mini_batches\n",
    "    \n",
    "    \n",
    "    def train_nn_MBGD(self, X_train, y_train, bs=100, iter_num=50, eta=0.01, lam=0.0500):\n",
    "        # self.back_prop(X_train[0], y_train[0])\n",
    "        cnt = 0\n",
    "        m = len(y_train)\n",
    "        avg_cost_func = []\n",
    "        tri_W, tri_b = self.W, self.b\n",
    "        print('Starting gradient descent for {} iterations'.format(iter_num))\n",
    "        while cnt < iter_num:\n",
    "            if cnt % 10 == 0:\n",
    "                print('Iteration {} of {}'.format(cnt, iter_num))\n",
    "            avg_cost = 0\n",
    "            mini_batches = self.get_mini_batches(X_train, y_train, bs)\n",
    "            for mb in mini_batches:\n",
    "                X_mb = mb[0]\n",
    "                y_mb = mb[1]\n",
    "                delta_c = np.zeros(shape=(4,))\n",
    "                # pdb.set_trace()\n",
    "                for i in range(len(y_mb)):\n",
    "                    # TODO: back prop to get derivatives\n",
    "                    self.forward_prop(X_mb[i])\n",
    "\n",
    "                    # TODO: compute deltas on output layer\n",
    "\n",
    "                    delta_c = self.a[-1] - y_mb[i]\n",
    "                    de = np.multiply(delta_c, self.g_prime(self.z[-1]))\n",
    "                    self.delta[self.L - 1] = de\n",
    "                    s = 0\n",
    "                    for w in self.W:\n",
    "                        s += np.sum(np.square(w))\n",
    "                    avg_cost += np.linalg.norm(de) + (lam / (2*bs)) * s\n",
    "\n",
    "                    # TODO: loop backward through layers, backprop deltas, compute dWs and dbs\n",
    "                    for ll in range(self.L - 2, -1, -1):\n",
    "                        self.dW[ll] += np.multiply(self.delta[ll + 1][:, np.newaxis], self.a[ll][:, np.newaxis].T)\n",
    "                        self.db[ll] += self.delta[ll + 1]\n",
    "                        temp = np.dot(self.W[ll].T, self.delta[ll + 1])\n",
    "                        temp1 = self.g_prime(self.z[ll])\n",
    "                        self.delta[ll] = np.multiply(temp, temp1)\n",
    "\n",
    "                    # avg_cost += self.accuracy(X_mb, y_mb)\n",
    "                for l in range(self.L - 2, -1, -1):\n",
    "                    w = self.W\n",
    "                    self.W[l] += -eta * (self.dW[l]  + lam * w[l])\n",
    "                    self.b[l] += -eta * (self.db[l])\n",
    "\n",
    "                if ((cnt + 1) % 5) == 1:\n",
    "                    #                 self.epoch_report(ep, num_epochs, X_train, y_train, X_valid, y_valid)\n",
    "                    self.epoch_accuracy[cnt] = self.accuracy(X_mb, y_mb)\n",
    "            # complete the average cost calculation\n",
    "            avg_cost = 1.0 / m * avg_cost\n",
    "            avg_cost_func.append(avg_cost)\n",
    "            cnt += 1\n",
    "        return avg_cost_func\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting gradient descent for 50 iterations\n",
      "Iteration 0 of 50\n",
      "Iteration 10 of 50\n",
      "Iteration 20 of 50\n",
      "Iteration 30 of 50\n",
      "Iteration 40 of 50\n",
      "{0: 0.3, 5: 0.32, 10: 0.23, 15: 0.3, 20: 0.28, 25: 0.22, 30: 0.25, 35: 0.24, 40: 0.31, 45: 0.2}\n"
     ]
    }
   ],
   "source": [
    "nn = Network_mini([441,100,4])\n",
    "rel_errs = nn.train_nn_MBGD(X_train, y_train)\n",
    "# rel_errs = nn.gradient_checking(X_train, y_train)\n",
    "print(nn.epoch_accuracy)\n",
    "\"\"\"\n",
    "for eta = 0.1\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting gradient descent for 50 iterations\n",
      "Iteration 0 of 50\n",
      "Iteration 10 of 50\n",
      "Iteration 20 of 50\n",
      "Iteration 30 of 50\n",
      "Iteration 40 of 50\n",
      "{0: 0.47, 5: 0.56, 10: 0.68, 15: 0.7, 20: 0.76, 25: 0.76, 30: 0.7, 35: 0.59, 40: 0.71, 45: 0.68}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nfor eta = 0.01\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn = Network_mini([441,100,4])\n",
    "rel_errs = nn.train_nn_MBGD(X_train, y_train)\n",
    "# rel_errs = nn.gradient_checking(X_train, y_train)\n",
    "print(nn.epoch_accuracy)\n",
    "\"\"\"\n",
    "for eta = 0.01\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Here you can see the smaller eta gives better results with lam = 0.05\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Code for tanh GD\n",
    "\"\"\"\n",
    "from numpy import random\n",
    "class Network_tanh:\n",
    "    def __init__(self, sizes):\n",
    "        \"\"\"\n",
    "        Initialize the neural network \n",
    "        \n",
    "        :param sizes: a list of the number of neurons in each layer \n",
    "        \"\"\"\n",
    "        # save the number of layers in the network \n",
    "        self.L = len(sizes) \n",
    "        \n",
    "        # store the list of layer sizes \n",
    "        self.sizes = sizes  \n",
    "        \n",
    "        # initialize the bias vectors for each hidden and output layer \n",
    "        self.b = [np.random.randn(n) for n in self.sizes[1:]]\n",
    "        \n",
    "        # initialize the matrices of weights for each hidden and output layer \n",
    "        self.W = [np.random.randn(n, m) for (m,n) in zip(self.sizes[:-1], self.sizes[1:])]\n",
    "        \n",
    "        # initialize the derivatives of biases for backprop \n",
    "        self.db = [np.zeros(n) for n in self.sizes[1:]]\n",
    "        \n",
    "        # initialize the derivatives of weights for backprop \n",
    "        self.dW = [np.zeros((n, m)) for (m,n) in zip(self.sizes[:-1], self.sizes[1:])]\n",
    "        \n",
    "        # initialize the activities on each hidden and output layer \n",
    "        self.z = [np.zeros(n) for n in self.sizes]\n",
    "        \n",
    "        # initialize the activations on each hidden and output layer \n",
    "        self.a = [np.zeros(n) for n in self.sizes]\n",
    "        \n",
    "        # initialize the deltas on each hidden and output layer \n",
    "        self.delta = [np.zeros(n) for n in self.sizes]\n",
    "        \n",
    "        #epoch accuracy dict\n",
    "        self.epoch_accuracy = {}\n",
    "        \n",
    "        # accuracy \n",
    "        self.acc_valid = []\n",
    "        self.acc_train = []\n",
    "        \n",
    "    def g(self, z):\n",
    "        \"\"\"\n",
    "        sigmoid activation function \n",
    "        \n",
    "        :param z: vector of activities to apply activation to \n",
    "        \"\"\"\n",
    "        z = np.clip(z, -20, 20)\n",
    "        return np.tanh(z)\n",
    "    \n",
    "    def g_prime(self, z):\n",
    "        \"\"\"\n",
    "        derivative of sigmoid activation function \n",
    "        \n",
    "        :param z: vector of activities to apply derivative of activation to \n",
    "        \"\"\"\n",
    "        return self.g(z) * (1.0 - self.g(z))\n",
    "    \n",
    "    def C(self, a, y):\n",
    "        \"\"\"\n",
    "        evaluate the cost function for squared-loss C(a,y) = ||a-y||^2/2 \n",
    "        \n",
    "        :param a: activations on output layer \n",
    "        :param y: vector-encoded label \n",
    "        \"\"\"\n",
    "        return 0.5 * np.linalg.norm(a - y)**2\n",
    "    \n",
    "    def gradC(self, a, y):\n",
    "        \"\"\"\n",
    "        evaluate gradient of cost function for squared-loss C(a,y) = ||a-y||^2/2 \n",
    "        \n",
    "        :param a: activations on output layer \n",
    "        :param y: vector-encoded label \n",
    "        \"\"\"\n",
    "        return (a - y)\n",
    "    \n",
    "    def forward_prop(self, x):\n",
    "        \"\"\"\n",
    "        take an feature vector and propagate through network \n",
    "        \n",
    "        :param x: input feature vector \n",
    "        \"\"\"\n",
    "        \n",
    "        # TODO: Initialize activation on initial layer to x \n",
    "        self.a[0] = copy.deepcopy(x)\n",
    "        \n",
    "        # TODO: Loop over layers and compute activities and activations \n",
    "        for l in range(self.L - 1):\n",
    "            w = self.W[l]\n",
    "            b = self.b[l]\n",
    "            self.z[l+1] = np.add(np.dot(w, self.a[l]), b)\n",
    "            self.a[l+1] = self.g(self.z[l+1])\n",
    "                \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predicts on the the data in X. Assume at least two output neurons so predictions\n",
    "        are one-hot encoded vectorized labels. \n",
    "        \n",
    "        :param X: a matrix of data to make predictions on \n",
    "        :return y: a matrix of vectorized labels \n",
    "        \"\"\"\n",
    "        yhat = np.zeros((X.shape[0], self.sizes[-1]), dtype=int)\n",
    "        \n",
    "        for x, j in enumerate(X):\n",
    "            self.forward_prop(j)\n",
    "            a = np.argmax(self.a[-1])\n",
    "            yhat[x][a] = 1            \n",
    "                \n",
    "        return yhat \n",
    "    \n",
    "    def accuracy(self, X, y):\n",
    "        \"\"\"\n",
    "        compute accuracy on labeled training set \n",
    "\n",
    "        :param X: matrix of features \n",
    "        :param y: matrix of vectorized true labels \n",
    "        \"\"\"\n",
    "        yhat = self.predict(X)\n",
    "        return np.sum(np.all(np.equal(yhat, y), axis=1)) / X.shape[0]\n",
    "            \n",
    "            \n",
    "    def back_prop(self, x, y):\n",
    "        \"\"\"\n",
    "        Back propagation to get derivatives of C wrt weights and biases for given training example\n",
    "        \n",
    "        :param x: training features  \n",
    "        :param y: vector-encoded label \n",
    "        \"\"\"\n",
    "        \n",
    "        # TODO: forward prop training example to fill in activities and activations\n",
    "        self.forward_prop(x)\n",
    "\n",
    "        # TODO: compute deltas on output layer\n",
    "        delta_c = self.a[-1] - y\n",
    "        de = np.multiply(delta_c, self.g_prime(self.z[-1]))\n",
    "        self.delta[self.L - 1] = de\n",
    "\n",
    "        # TODO: loop backward through layers, backprop deltas, compute dWs and dbs\n",
    "        for ll in range(self.L - 2, -1, -1):\n",
    "            self.dW[ll] = np.multiply(self.delta[ll + 1][:, np.newaxis], self.a[ll][:, np.newaxis].T)\n",
    "            self.db[ll] = self.delta[ll + 1]\n",
    "            temp = np.dot(self.W[ll].T, self.delta[ll + 1])\n",
    "            temp1 = self.g_prime(self.z[ll])\n",
    "            self.delta[ll] = np.multiply(temp, temp1)\n",
    "            \n",
    "        \n",
    "    def gradient_checking(self, X_train, y_train, EPS=0.0001):\n",
    "        \"\"\"\n",
    "        Performs gradient checking on all weights in the \n",
    "        network for a randomly selected training example \n",
    "        :param X_train: matrix of training features \n",
    "        :param y_train: matrix of vector-encoded training labels \n",
    "        \"\"\"\n",
    "        # Randomly select a training example \n",
    "        kk = np.random.randint(0,X_train.shape[0])\n",
    "        xk = X_train[kk]\n",
    "        yk = y_train[kk]\n",
    "\n",
    "        # Get the analytic(ish) weights from back_prop \n",
    "        self.back_prop(xk, yk)\n",
    "\n",
    "        # List of relative errors.  Used only for unit testing. \n",
    "        rel_errors = []\n",
    "\n",
    "        # Loop over and perturb each weight/bias in \n",
    "        # network and test numerical derivative \n",
    "        # Don't forget that after perturbing the weights\n",
    "        # you'll want to put them back the way they were! \n",
    "        \n",
    "        # Loop over and perturb each weight/bias in \n",
    "        # network and test numerical derivative \n",
    "        for ell in range(self.L-1):\n",
    "            for ii in range(self.W[ell].shape[0]):\n",
    "                # Check weights in level W[ell][ii,jj] \n",
    "                for jj in range(self.W[ell].shape[1]):\n",
    "                    \n",
    "                    # TODO true_dW \n",
    "                    true_dW = self.dW[ell][ii,jj]\n",
    "                    # TODO num_dW \n",
    "                    true_W = copy.deepcopy(self.W[ell][ii,jj])\n",
    "                    self.W[ell][ii,jj] = true_W + EPS\n",
    "                    self.forward_prop(xk)\n",
    "                    l1 = self.C(self.a[-1],yk)\n",
    "                    self.W[ell][ii,jj] = true_W - EPS\n",
    "                    self.forward_prop(xk)\n",
    "                    l2 = self.C(self.a[-1],yk)\n",
    "                    num_dW = (l1 - l2) / (2*EPS)\n",
    "                    \n",
    "                    self.W[ell][ii,jj] = true_W\n",
    "                    \n",
    "                    rel_dW = np.abs(true_dW-num_dW)/np.abs(true_dW)\n",
    "                    print(\"W[{:d}][{:d},{:d}]: true: {: 12.10e}  approx: {: 12.10e} rel_err: {: 12.10e}\".format(ell, ii, jj, true_dW, num_dW, rel_dW))\n",
    "                    rel_errors.append(rel_dW)\n",
    "                                    \n",
    "                # TODO num_db  \n",
    "                true_db = self.db[ell][ii]\n",
    "                true_b = copy.deepcopy(self.b[ell][ii])\n",
    "                self.b[ell][ii] = true_b + EPS\n",
    "                self.forward_prop(xk)\n",
    "                l1 = self.C(self.a[-1],yk)\n",
    "                self.b[ell][ii] = true_b - EPS\n",
    "                self.forward_prop(xk)\n",
    "                l2 = self.C(self.a[-1],yk)\n",
    "                num_db = (l1 - l2) / (2*EPS)\n",
    "                self.b[ell][ii] = true_b\n",
    "                \n",
    "                \n",
    "                rel_db = np.abs(true_db-num_db)/np.abs(true_db)\n",
    "                print(\"b[{:d}][{:d}]:   true: {: 12.10e}  approx: {: 12.10e} rel_err: {: 12.10e}\".format(ell, ii, true_db, num_db, rel_db))\n",
    "                rel_errors.append(rel_db)\n",
    "\n",
    "        return rel_errors\n",
    "            \n",
    "            \n",
    "    def train(self, X_train, y_train, X_valid=None, y_valid=None, eta=0.25, lam=0.0, num_epochs=10, isPrint=True):\n",
    "        \"\"\"\n",
    "        Train the network with SGD \n",
    "        \n",
    "        :param X_train: matrix of training features \n",
    "        :param y_train: matrix of vector-encoded training labels \n",
    "        :param X_train: optional matrix of validation features \n",
    "        :param y_train: optional matrix of vector-encoded validation labels \n",
    "        :param eta: learning rate \n",
    "        :param lam: regularization strength \n",
    "        :param num_epochs: number of epochs to run \n",
    "        :param isPrint: flag indicating to print training progress or not \n",
    "        \"\"\"\n",
    "        \n",
    "        import copy\n",
    "        \n",
    "        # initialize shuffled indices \n",
    "        shuffled_inds = list(range(X_train.shape[0]))\n",
    "        \n",
    "        # loop over training epochs \n",
    "        for ep in range(num_epochs):\n",
    "            \n",
    "            # shuffle indices \n",
    "            np.random.shuffle(shuffled_inds)\n",
    "            \n",
    "            # loop over training examples \n",
    "            for ind in shuffled_inds:\n",
    "                \n",
    "                self.back_prop(X_train[ind],y_train[ind]) \n",
    "                dW = copy.deepcopy(self.dW)\n",
    "                sW = copy.deepcopy(self.W)\n",
    "                db = copy.deepcopy(self.db)\n",
    "                self.W = self.W - np.multiply(eta, dW) - np.multiply(sW,lam*eta)\n",
    "                self.b = self.b - np.multiply(eta, db)\n",
    "                \n",
    "            # occasionally print accuracy\n",
    "            if isPrint and ((ep+1)%5)==1:\n",
    "#                 self.epoch_report(ep, num_epochs, X_train, y_train, X_valid, y_valid)\n",
    "                self.epoch_accuracy[ep] = self.accuracy(X_valid, y_valid)\n",
    "                \n",
    "        # print final accuracy\n",
    "        if isPrint:\n",
    "            self.epoch_report(ep, num_epochs, X_train, y_train, X_valid, y_valid)\n",
    "                \n",
    "                    \n",
    "    def epoch_report(self, ep, num_epochs, X_train, y_train, X_valid, y_valid):\n",
    "        \"\"\"\n",
    "        Print the accuracy for the given epoch on training and validation data \n",
    "        \n",
    "        :param ep: the current epoch \n",
    "        :param num_epochs: the total number of epochs\n",
    "        :param X_train: matrix of training features \n",
    "        :param y_train: matrix of vector-encoded training labels \n",
    "        :param X_train: optional matrix of validation features \n",
    "        :param y_train: optional matrix of vector-encoded validation labels \n",
    "        \"\"\"\n",
    "        print(\"epoch {:3d}/{:3d}: \".format(ep+1, num_epochs), end=\"\")\n",
    "        print(\"  train acc: {:8.3f}\".format(self.accuracy(X_train, y_train)), end=\"\")\n",
    "        if X_valid is not None: \n",
    "            print(\"  valid acc: {:8.3f}\".format(self.accuracy(X_valid, y_valid)))\n",
    "            \n",
    "        else: print(\"\")  \n",
    "            \n",
    "    def get_mini_batches(self, X, y, batch_size):\n",
    "        random_idxs = random.choice(len(y), len(y), replace=False)\n",
    "        X_shuffled = X[random_idxs, :]\n",
    "        y_shuffled = y[random_idxs]\n",
    "        mini_batches = [(X_shuffled[i:i + batch_size, :], y_shuffled[i:i + batch_size]) for\n",
    "                        i in range(0, len(y), batch_size)]\n",
    "        return mini_batches\n",
    "    \n",
    "    \n",
    "    def train_nn_MBGD(self, X_train, y_train, bs=100, iter_num=50, eta=0.1, lam=0.0500):\n",
    "        # self.back_prop(X_train[0], y_train[0])\n",
    "        cnt = 0\n",
    "        m = len(y_train)\n",
    "        avg_cost_func = []\n",
    "        tri_W, tri_b = self.W, self.b\n",
    "        print('Starting gradient descent for {} iterations'.format(iter_num))\n",
    "        while cnt < iter_num:\n",
    "            if cnt % 10 == 0:\n",
    "                print('Iteration {} of {}'.format(cnt, iter_num))\n",
    "            avg_cost = 0\n",
    "            mini_batches = self.get_mini_batches(X_train, y_train, bs)\n",
    "            for mb in mini_batches:\n",
    "                X_mb = mb[0]\n",
    "                y_mb = mb[1]\n",
    "                delta_c = np.zeros(shape=(4,))\n",
    "                # pdb.set_trace()\n",
    "                for i in range(len(y_mb)):\n",
    "                    # TODO: back prop to get derivatives\n",
    "                    self.forward_prop(X_mb[i])\n",
    "\n",
    "                    # TODO: compute deltas on output layer\n",
    "\n",
    "                    delta_c = self.a[-1] - y_mb[i]\n",
    "                    de = np.multiply(delta_c, self.g_prime(self.z[-1]))\n",
    "                    self.delta[self.L - 1] = de\n",
    "                    s = 0\n",
    "                    for w in self.W:\n",
    "                        s += np.sum(np.square(w))\n",
    "                    avg_cost += np.linalg.norm(de) + (lam / (2*bs)) * s\n",
    "\n",
    "                    # TODO: loop backward through layers, backprop deltas, compute dWs and dbs\n",
    "                    for ll in range(self.L - 2, -1, -1):\n",
    "                        self.dW[ll] += np.multiply(self.delta[ll + 1][:, np.newaxis], self.a[ll][:, np.newaxis].T)\n",
    "                        self.db[ll] += self.delta[ll + 1]\n",
    "                        temp = np.dot(self.W[ll].T, self.delta[ll + 1])\n",
    "                        temp1 = self.g_prime(self.z[ll])\n",
    "                        self.delta[ll] = np.multiply(temp, temp1)\n",
    "\n",
    "                    # avg_cost += self.accuracy(X_mb, y_mb)\n",
    "                for l in range(self.L - 2, -1, -1):\n",
    "                    w = self.W\n",
    "                    self.W[l] += -eta * (self.dW[l]  + lam * w[l])\n",
    "                    self.b[l] += -eta * (self.db[l])\n",
    "\n",
    "                if ((cnt + 1) % 5) == 1:\n",
    "                    #                 self.epoch_report(ep, num_epochs, X_train, y_train, X_valid, y_valid)\n",
    "                    self.epoch_accuracy[cnt] = self.accuracy(X_mb, y_mb)\n",
    "            # complete the average cost calculation\n",
    "            avg_cost = 1.0 / m * avg_cost\n",
    "            avg_cost_func.append(avg_cost)\n",
    "            cnt += 1\n",
    "        return avg_cost_func\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting gradient descent for 50 iterations\n",
      "Iteration 0 of 50\n",
      "Iteration 10 of 50\n",
      "Iteration 20 of 50\n",
      "Iteration 30 of 50\n",
      "Iteration 40 of 50\n",
      "{0: 0.23, 5: 0.28, 10: 0.27, 15: 0.21, 20: 0.28, 25: 0.17, 30: 0.2, 35: 0.22, 40: 0.29, 45: 0.17}\n"
     ]
    }
   ],
   "source": [
    "nn = Network_tanh([441,100,4])\n",
    "rel_errs = nn.train_nn_MBGD(X_train, y_train)\n",
    "# rel_errs = nn.gradient_checking(X_train, y_train)\n",
    "print(nn.epoch_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Signoid has better accuracy than tanh\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Code for relu GD\n",
    "\"\"\"\n",
    "from numpy import random\n",
    "class Network_relu:\n",
    "    def __init__(self, sizes):\n",
    "        \"\"\"\n",
    "        Initialize the neural network \n",
    "        \n",
    "        :param sizes: a list of the number of neurons in each layer \n",
    "        \"\"\"\n",
    "        # save the number of layers in the network \n",
    "        self.L = len(sizes) \n",
    "        \n",
    "        # store the list of layer sizes \n",
    "        self.sizes = sizes  \n",
    "        \n",
    "        # initialize the bias vectors for each hidden and output layer \n",
    "        self.b = [np.random.randn(n) for n in self.sizes[1:]]\n",
    "        \n",
    "        # initialize the matrices of weights for each hidden and output layer \n",
    "        self.W = [np.random.randn(n, m) for (m,n) in zip(self.sizes[:-1], self.sizes[1:])]\n",
    "        \n",
    "        # initialize the derivatives of biases for backprop \n",
    "        self.db = [np.zeros(n) for n in self.sizes[1:]]\n",
    "        \n",
    "        # initialize the derivatives of weights for backprop \n",
    "        self.dW = [np.zeros((n, m)) for (m,n) in zip(self.sizes[:-1], self.sizes[1:])]\n",
    "        \n",
    "        # initialize the activities on each hidden and output layer \n",
    "        self.z = [np.zeros(n) for n in self.sizes]\n",
    "        \n",
    "        # initialize the activations on each hidden and output layer \n",
    "        self.a = [np.zeros(n) for n in self.sizes]\n",
    "        \n",
    "        # initialize the deltas on each hidden and output layer \n",
    "        self.delta = [np.zeros(n) for n in self.sizes]\n",
    "        \n",
    "        #epoch accuracy dict\n",
    "        self.epoch_accuracy = {}\n",
    "        \n",
    "        # accuracy \n",
    "        self.acc_valid = []\n",
    "        self.acc_train = []\n",
    "        \n",
    "    def g(self, z):\n",
    "        \"\"\"\n",
    "        sigmoid activation function \n",
    "        \n",
    "        :param z: vector of activities to apply activation to \n",
    "        \"\"\"\n",
    "        \n",
    "        z = np.clip(z, -20, 20)\n",
    "        return np.maximum(z, 0)\n",
    "    \n",
    "    def g_prime(self, z):\n",
    "        \"\"\"\n",
    "        derivative of sigmoid activation function \n",
    "        \n",
    "        :param z: vector of activities to apply derivative of activation to \n",
    "        \"\"\"\n",
    "        z[z<=0] = 0.00\n",
    "        z[z>0] = 1\n",
    "        return z\n",
    "    \n",
    "    def C(self, a, y):\n",
    "        \"\"\"\n",
    "        evaluate the cost function for squared-loss C(a,y) = ||a-y||^2/2 \n",
    "        \n",
    "        :param a: activations on output layer \n",
    "        :param y: vector-encoded label \n",
    "        \"\"\"\n",
    "        return 0.5 * np.linalg.norm(a - y)**2\n",
    "    \n",
    "    def gradC(self, a, y):\n",
    "        \"\"\"\n",
    "        evaluate gradient of cost function for squared-loss C(a,y) = ||a-y||^2/2 \n",
    "        \n",
    "        :param a: activations on output layer \n",
    "        :param y: vector-encoded label \n",
    "        \"\"\"\n",
    "        return (a - y)\n",
    "    \n",
    "    def forward_prop(self, x):\n",
    "        \"\"\"\n",
    "        take an feature vector and propagate through network \n",
    "        \n",
    "        :param x: input feature vector \n",
    "        \"\"\"\n",
    "        \n",
    "        # TODO: Initialize activation on initial layer to x \n",
    "        self.a[0] = copy.deepcopy(x)\n",
    "        \n",
    "        # TODO: Loop over layers and compute activities and activations \n",
    "        for l in range(self.L - 1):\n",
    "            w = self.W[l]\n",
    "            b = self.b[l]\n",
    "            self.z[l+1] = np.add(np.dot(w, self.a[l]), b)\n",
    "            self.a[l+1] = self.g(self.z[l+1])\n",
    "                \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predicts on the the data in X. Assume at least two output neurons so predictions\n",
    "        are one-hot encoded vectorized labels. \n",
    "        \n",
    "        :param X: a matrix of data to make predictions on \n",
    "        :return y: a matrix of vectorized labels \n",
    "        \"\"\"\n",
    "        yhat = np.zeros((X.shape[0], self.sizes[-1]), dtype=int)\n",
    "        \n",
    "        for x, j in enumerate(X):\n",
    "            self.forward_prop(j)\n",
    "            a = np.argmax(self.a[-1])\n",
    "            yhat[x][a] = 1            \n",
    "                \n",
    "        return yhat \n",
    "    \n",
    "    def accuracy(self, X, y):\n",
    "        \"\"\"\n",
    "        compute accuracy on labeled training set \n",
    "\n",
    "        :param X: matrix of features \n",
    "        :param y: matrix of vectorized true labels \n",
    "        \"\"\"\n",
    "        yhat = self.predict(X)\n",
    "        return np.sum(np.all(np.equal(yhat, y), axis=1)) / X.shape[0]\n",
    "            \n",
    "            \n",
    "    def back_prop(self, x, y):\n",
    "        \"\"\"\n",
    "        Back propagation to get derivatives of C wrt weights and biases for given training example\n",
    "        \n",
    "        :param x: training features  \n",
    "        :param y: vector-encoded label \n",
    "        \"\"\"\n",
    "        \n",
    "        # TODO: forward prop training example to fill in activities and activations\n",
    "        self.forward_prop(x)\n",
    "\n",
    "        # TODO: compute deltas on output layer\n",
    "        delta_c = self.a[-1] - y\n",
    "        de = np.multiply(delta_c, self.g_prime(self.z[-1]))\n",
    "        self.delta[self.L - 1] = de\n",
    "\n",
    "        # TODO: loop backward through layers, backprop deltas, compute dWs and dbs\n",
    "        for ll in range(self.L - 2, -1, -1):\n",
    "            self.dW[ll] = np.multiply(self.delta[ll + 1][:, np.newaxis], self.a[ll][:, np.newaxis].T)\n",
    "            self.db[ll] = self.delta[ll + 1]\n",
    "            temp = np.dot(self.W[ll].T, self.delta[ll + 1])\n",
    "            temp1 = self.g_prime(self.z[ll])\n",
    "            self.delta[ll] = np.multiply(temp, temp1)\n",
    "            \n",
    "        \n",
    "    def gradient_checking(self, X_train, y_train, EPS=0.0001):\n",
    "        \"\"\"\n",
    "        Performs gradient checking on all weights in the \n",
    "        network for a randomly selected training example \n",
    "        :param X_train: matrix of training features \n",
    "        :param y_train: matrix of vector-encoded training labels \n",
    "        \"\"\"\n",
    "        # Randomly select a training example \n",
    "        kk = np.random.randint(0,X_train.shape[0])\n",
    "        xk = X_train[kk]\n",
    "        yk = y_train[kk]\n",
    "\n",
    "        # Get the analytic(ish) weights from back_prop \n",
    "        self.back_prop(xk, yk)\n",
    "\n",
    "        # List of relative errors.  Used only for unit testing. \n",
    "        rel_errors = []\n",
    "\n",
    "        # Loop over and perturb each weight/bias in \n",
    "        # network and test numerical derivative \n",
    "        # Don't forget that after perturbing the weights\n",
    "        # you'll want to put them back the way they were! \n",
    "        \n",
    "        # Loop over and perturb each weight/bias in \n",
    "        # network and test numerical derivative \n",
    "        for ell in range(self.L-1):\n",
    "            for ii in range(self.W[ell].shape[0]):\n",
    "                # Check weights in level W[ell][ii,jj] \n",
    "                for jj in range(self.W[ell].shape[1]):\n",
    "                    \n",
    "                    # TODO true_dW \n",
    "                    true_dW = self.dW[ell][ii,jj]\n",
    "                    # TODO num_dW \n",
    "                    true_W = copy.deepcopy(self.W[ell][ii,jj])\n",
    "                    self.W[ell][ii,jj] = true_W + EPS\n",
    "                    self.forward_prop(xk)\n",
    "                    l1 = self.C(self.a[-1],yk)\n",
    "                    self.W[ell][ii,jj] = true_W - EPS\n",
    "                    self.forward_prop(xk)\n",
    "                    l2 = self.C(self.a[-1],yk)\n",
    "                    num_dW = (l1 - l2) / (2*EPS)\n",
    "                    \n",
    "                    self.W[ell][ii,jj] = true_W\n",
    "                    \n",
    "                    rel_dW = np.abs(true_dW-num_dW)/np.abs(true_dW)\n",
    "                    print(\"W[{:d}][{:d},{:d}]: true: {: 12.10e}  approx: {: 12.10e} rel_err: {: 12.10e}\".format(ell, ii, jj, true_dW, num_dW, rel_dW))\n",
    "                    rel_errors.append(rel_dW)\n",
    "                                    \n",
    "                # TODO num_db  \n",
    "                true_db = self.db[ell][ii]\n",
    "                true_b = copy.deepcopy(self.b[ell][ii])\n",
    "                self.b[ell][ii] = true_b + EPS\n",
    "                self.forward_prop(xk)\n",
    "                l1 = self.C(self.a[-1],yk)\n",
    "                self.b[ell][ii] = true_b - EPS\n",
    "                self.forward_prop(xk)\n",
    "                l2 = self.C(self.a[-1],yk)\n",
    "                num_db = (l1 - l2) / (2*EPS)\n",
    "                self.b[ell][ii] = true_b\n",
    "                \n",
    "                \n",
    "                rel_db = np.abs(true_db-num_db)/np.abs(true_db)\n",
    "                print(\"b[{:d}][{:d}]:   true: {: 12.10e}  approx: {: 12.10e} rel_err: {: 12.10e}\".format(ell, ii, true_db, num_db, rel_db))\n",
    "                rel_errors.append(rel_db)\n",
    "\n",
    "        return rel_errors\n",
    "            \n",
    "            \n",
    "    def train(self, X_train, y_train, X_valid=None, y_valid=None, eta=0.25, lam=0.0, num_epochs=10, isPrint=True):\n",
    "        \"\"\"\n",
    "        Train the network with SGD \n",
    "        \n",
    "        :param X_train: matrix of training features \n",
    "        :param y_train: matrix of vector-encoded training labels \n",
    "        :param X_train: optional matrix of validation features \n",
    "        :param y_train: optional matrix of vector-encoded validation labels \n",
    "        :param eta: learning rate \n",
    "        :param lam: regularization strength \n",
    "        :param num_epochs: number of epochs to run \n",
    "        :param isPrint: flag indicating to print training progress or not \n",
    "        \"\"\"\n",
    "        \n",
    "        import copy\n",
    "        \n",
    "        # initialize shuffled indices \n",
    "        shuffled_inds = list(range(X_train.shape[0]))\n",
    "        \n",
    "        # loop over training epochs \n",
    "        for ep in range(num_epochs):\n",
    "            \n",
    "            # shuffle indices \n",
    "            np.random.shuffle(shuffled_inds)\n",
    "            \n",
    "            # loop over training examples \n",
    "            for ind in shuffled_inds:\n",
    "                \n",
    "                self.back_prop(X_train[ind],y_train[ind]) \n",
    "                dW = copy.deepcopy(self.dW)\n",
    "                sW = copy.deepcopy(self.W)\n",
    "                db = copy.deepcopy(self.db)\n",
    "                self.W = self.W - np.multiply(eta, dW) - np.multiply(sW,lam*eta)\n",
    "                self.b = self.b - np.multiply(eta, db)\n",
    "                \n",
    "            # occasionally print accuracy\n",
    "            if isPrint and ((ep+1)%5)==1:\n",
    "#                 self.epoch_report(ep, num_epochs, X_train, y_train, X_valid, y_valid)\n",
    "                self.epoch_accuracy[ep] = self.accuracy(X_valid, y_valid)\n",
    "                \n",
    "        # print final accuracy\n",
    "        if isPrint:\n",
    "            self.epoch_report(ep, num_epochs, X_train, y_train, X_valid, y_valid)\n",
    "                \n",
    "                    \n",
    "    def epoch_report(self, ep, num_epochs, X_train, y_train, X_valid, y_valid):\n",
    "        \"\"\"\n",
    "        Print the accuracy for the given epoch on training and validation data \n",
    "        \n",
    "        :param ep: the current epoch \n",
    "        :param num_epochs: the total number of epochs\n",
    "        :param X_train: matrix of training features \n",
    "        :param y_train: matrix of vector-encoded training labels \n",
    "        :param X_train: optional matrix of validation features \n",
    "        :param y_train: optional matrix of vector-encoded validation labels \n",
    "        \"\"\"\n",
    "        print(\"epoch {:3d}/{:3d}: \".format(ep+1, num_epochs), end=\"\")\n",
    "        print(\"  train acc: {:8.3f}\".format(self.accuracy(X_train, y_train)), end=\"\")\n",
    "        if X_valid is not None: \n",
    "            print(\"  valid acc: {:8.3f}\".format(self.accuracy(X_valid, y_valid)))\n",
    "            \n",
    "        else: print(\"\")  \n",
    "            \n",
    "    def get_mini_batches(self, X, y, batch_size):\n",
    "        random_idxs = random.choice(len(y), len(y), replace=False)\n",
    "        X_shuffled = X[random_idxs, :]\n",
    "        y_shuffled = y[random_idxs]\n",
    "        mini_batches = [(X_shuffled[i:i + batch_size, :], y_shuffled[i:i + batch_size]) for\n",
    "                        i in range(0, len(y), batch_size)]\n",
    "        return mini_batches\n",
    "    \n",
    "    \n",
    "    def train_nn_MBGD(self, X_train, y_train, bs=100, iter_num=50, eta=0.1, lam=0.0500):\n",
    "        # self.back_prop(X_train[0], y_train[0])\n",
    "        cnt = 0\n",
    "        m = len(y_train)\n",
    "        avg_cost_func = []\n",
    "        tri_W, tri_b = self.W, self.b\n",
    "        print('Starting gradient descent for {} iterations'.format(iter_num))\n",
    "        while cnt < iter_num:\n",
    "            if cnt % 10 == 0:\n",
    "                print('Iteration {} of {}'.format(cnt, iter_num))\n",
    "            avg_cost = 0\n",
    "            mini_batches = self.get_mini_batches(X_train, y_train, bs)\n",
    "            for mb in mini_batches:\n",
    "                X_mb = mb[0]\n",
    "                y_mb = mb[1]\n",
    "                delta_c = np.zeros(shape=(4,))\n",
    "                # pdb.set_trace()\n",
    "                for i in range(len(y_mb)):\n",
    "                    # TODO: back prop to get derivatives\n",
    "                    self.forward_prop(X_mb[i])\n",
    "\n",
    "                    # TODO: compute deltas on output layer\n",
    "\n",
    "                    delta_c = self.a[-1] - y_mb[i]\n",
    "                    de = np.multiply(delta_c, self.g_prime(self.z[-1]))\n",
    "                    self.delta[self.L - 1] = de\n",
    "                    s = 0\n",
    "                    for w in self.W:\n",
    "                        s += np.sum(np.square(w))\n",
    "                    avg_cost += np.linalg.norm(de) + (lam / (2*bs)) * s\n",
    "\n",
    "                    # TODO: loop backward through layers, backprop deltas, compute dWs and dbs\n",
    "                    for ll in range(self.L - 2, -1, -1):\n",
    "                        self.dW[ll] += np.multiply(self.delta[ll + 1][:, np.newaxis], self.a[ll][:, np.newaxis].T)\n",
    "                        self.db[ll] += self.delta[ll + 1]\n",
    "                        temp = np.dot(self.W[ll].T, self.delta[ll + 1])\n",
    "                        temp1 = self.g_prime(self.z[ll])\n",
    "                        self.delta[ll] = np.multiply(temp, temp1)\n",
    "\n",
    "                    # avg_cost += self.accuracy(X_mb, y_mb)\n",
    "                for l in range(self.L - 2, -1, -1):\n",
    "                    w = self.W\n",
    "                    self.W[l] += -eta * (self.dW[l]  + lam * w[l])\n",
    "                    self.b[l] += -eta * (self.db[l])\n",
    "\n",
    "                if ((cnt + 1) % 5) == 1:\n",
    "                    #                 self.epoch_report(ep, num_epochs, X_train, y_train, X_valid, y_valid)\n",
    "                    self.epoch_accuracy[cnt] = self.accuracy(X_mb, y_mb)\n",
    "            # complete the average cost calculation\n",
    "            avg_cost = 1.0 / m * avg_cost\n",
    "            avg_cost_func.append(avg_cost)\n",
    "            cnt += 1\n",
    "        return avg_cost_func\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting gradient descent for 50 iterations\n",
      "Iteration 0 of 50\n",
      "Iteration 10 of 50\n",
      "Iteration 20 of 50\n",
      "Iteration 30 of 50\n",
      "Iteration 40 of 50\n",
      "{0: 0.21, 5: 0.24, 10: 0.23, 15: 0.24, 20: 0.26, 25: 0.3, 30: 0.27, 35: 0.24, 40: 0.25, 45: 0.35}\n"
     ]
    }
   ],
   "source": [
    "nn = Network_relu([441,100,4])\n",
    "rel_errs = nn.train_nn_MBGD(X_train, y_train)\n",
    "# rel_errs = nn.gradient_checking(X_train, y_train)\n",
    "print(nn.epoch_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Code for relu GD\n",
    "\"\"\"\n",
    "from numpy import random\n",
    "class Network_leakyrelu:\n",
    "    def __init__(self, sizes):\n",
    "        \"\"\"\n",
    "        Initialize the neural network \n",
    "        \n",
    "        :param sizes: a list of the number of neurons in each layer \n",
    "        \"\"\"\n",
    "        # save the number of layers in the network \n",
    "        self.L = len(sizes) \n",
    "        \n",
    "        # store the list of layer sizes \n",
    "        self.sizes = sizes  \n",
    "        \n",
    "        # initialize the bias vectors for each hidden and output layer \n",
    "        self.b = [np.random.randn(n) for n in self.sizes[1:]]\n",
    "        \n",
    "        # initialize the matrices of weights for each hidden and output layer \n",
    "        self.W = [np.random.randn(n, m) for (m,n) in zip(self.sizes[:-1], self.sizes[1:])]\n",
    "        \n",
    "        # initialize the derivatives of biases for backprop \n",
    "        self.db = [np.zeros(n) for n in self.sizes[1:]]\n",
    "        \n",
    "        # initialize the derivatives of weights for backprop \n",
    "        self.dW = [np.zeros((n, m)) for (m,n) in zip(self.sizes[:-1], self.sizes[1:])]\n",
    "        \n",
    "        # initialize the activities on each hidden and output layer \n",
    "        self.z = [np.zeros(n) for n in self.sizes]\n",
    "        \n",
    "        # initialize the activations on each hidden and output layer \n",
    "        self.a = [np.zeros(n) for n in self.sizes]\n",
    "        \n",
    "        # initialize the deltas on each hidden and output layer \n",
    "        self.delta = [np.zeros(n) for n in self.sizes]\n",
    "        \n",
    "        #epoch accuracy dict\n",
    "        self.epoch_accuracy = {}\n",
    "        \n",
    "        # accuracy \n",
    "        self.acc_valid = []\n",
    "        self.acc_train = []\n",
    "        \n",
    "    def g(self, z):\n",
    "        \"\"\"\n",
    "        sigmoid activation function \n",
    "        \n",
    "        :param z: vector of activities to apply activation to \n",
    "        \"\"\"\n",
    "        \n",
    "        z = np.clip(z, -20, 20)\n",
    "        return np.maximum(z, 0.01*z)\n",
    "    \n",
    "    def g_prime(self, z):\n",
    "        \"\"\"\n",
    "        derivative of sigmoid activation function \n",
    "        \n",
    "        :param z: vector of activities to apply derivative of activation to \n",
    "        \"\"\"\n",
    "        z[z<=0] = 0.01\n",
    "        z[z>0] = 1\n",
    "        return z\n",
    "    \n",
    "    def C(self, a, y):\n",
    "        \"\"\"\n",
    "        evaluate the cost function for squared-loss C(a,y) = ||a-y||^2/2 \n",
    "        \n",
    "        :param a: activations on output layer \n",
    "        :param y: vector-encoded label \n",
    "        \"\"\"\n",
    "        return 0.5 * np.linalg.norm(a - y)**2\n",
    "    \n",
    "    def gradC(self, a, y):\n",
    "        \"\"\"\n",
    "        evaluate gradient of cost function for squared-loss C(a,y) = ||a-y||^2/2 \n",
    "        \n",
    "        :param a: activations on output layer \n",
    "        :param y: vector-encoded label \n",
    "        \"\"\"\n",
    "        return (a - y)\n",
    "    \n",
    "    def forward_prop(self, x):\n",
    "        \"\"\"\n",
    "        take an feature vector and propagate through network \n",
    "        \n",
    "        :param x: input feature vector \n",
    "        \"\"\"\n",
    "        \n",
    "        # TODO: Initialize activation on initial layer to x \n",
    "        self.a[0] = copy.deepcopy(x)\n",
    "        \n",
    "        # TODO: Loop over layers and compute activities and activations \n",
    "        for l in range(self.L - 1):\n",
    "            w = self.W[l]\n",
    "            b = self.b[l]\n",
    "            self.z[l+1] = np.add(np.dot(w, self.a[l]), b)\n",
    "            self.a[l+1] = self.g(self.z[l+1])\n",
    "                \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predicts on the the data in X. Assume at least two output neurons so predictions\n",
    "        are one-hot encoded vectorized labels. \n",
    "        \n",
    "        :param X: a matrix of data to make predictions on \n",
    "        :return y: a matrix of vectorized labels \n",
    "        \"\"\"\n",
    "        yhat = np.zeros((X.shape[0], self.sizes[-1]), dtype=int)\n",
    "        \n",
    "        for x, j in enumerate(X):\n",
    "            self.forward_prop(j)\n",
    "            a = np.argmax(self.a[-1])\n",
    "            yhat[x][a] = 1            \n",
    "                \n",
    "        return yhat \n",
    "    \n",
    "    def accuracy(self, X, y):\n",
    "        \"\"\"\n",
    "        compute accuracy on labeled training set \n",
    "\n",
    "        :param X: matrix of features \n",
    "        :param y: matrix of vectorized true labels \n",
    "        \"\"\"\n",
    "        yhat = self.predict(X)\n",
    "        return np.sum(np.all(np.equal(yhat, y), axis=1)) / X.shape[0]\n",
    "            \n",
    "            \n",
    "    def back_prop(self, x, y):\n",
    "        \"\"\"\n",
    "        Back propagation to get derivatives of C wrt weights and biases for given training example\n",
    "        \n",
    "        :param x: training features  \n",
    "        :param y: vector-encoded label \n",
    "        \"\"\"\n",
    "        \n",
    "        # TODO: forward prop training example to fill in activities and activations\n",
    "        self.forward_prop(x)\n",
    "\n",
    "        # TODO: compute deltas on output layer\n",
    "        delta_c = self.a[-1] - y\n",
    "        de = np.multiply(delta_c, self.g_prime(self.z[-1]))\n",
    "        self.delta[self.L - 1] = de\n",
    "\n",
    "        # TODO: loop backward through layers, backprop deltas, compute dWs and dbs\n",
    "        for ll in range(self.L - 2, -1, -1):\n",
    "            self.dW[ll] = np.multiply(self.delta[ll + 1][:, np.newaxis], self.a[ll][:, np.newaxis].T)\n",
    "            self.db[ll] = self.delta[ll + 1]\n",
    "            temp = np.dot(self.W[ll].T, self.delta[ll + 1])\n",
    "            temp1 = self.g_prime(self.z[ll])\n",
    "            self.delta[ll] = np.multiply(temp, temp1)\n",
    "            \n",
    "        \n",
    "    def gradient_checking(self, X_train, y_train, EPS=0.0001):\n",
    "        \"\"\"\n",
    "        Performs gradient checking on all weights in the \n",
    "        network for a randomly selected training example \n",
    "        :param X_train: matrix of training features \n",
    "        :param y_train: matrix of vector-encoded training labels \n",
    "        \"\"\"\n",
    "        # Randomly select a training example \n",
    "        kk = np.random.randint(0,X_train.shape[0])\n",
    "        xk = X_train[kk]\n",
    "        yk = y_train[kk]\n",
    "\n",
    "        # Get the analytic(ish) weights from back_prop \n",
    "        self.back_prop(xk, yk)\n",
    "\n",
    "        # List of relative errors.  Used only for unit testing. \n",
    "        rel_errors = []\n",
    "\n",
    "        # Loop over and perturb each weight/bias in \n",
    "        # network and test numerical derivative \n",
    "        # Don't forget that after perturbing the weights\n",
    "        # you'll want to put them back the way they were! \n",
    "        \n",
    "        # Loop over and perturb each weight/bias in \n",
    "        # network and test numerical derivative \n",
    "        for ell in range(self.L-1):\n",
    "            for ii in range(self.W[ell].shape[0]):\n",
    "                # Check weights in level W[ell][ii,jj] \n",
    "                for jj in range(self.W[ell].shape[1]):\n",
    "                    \n",
    "                    # TODO true_dW \n",
    "                    true_dW = self.dW[ell][ii,jj]\n",
    "                    # TODO num_dW \n",
    "                    true_W = copy.deepcopy(self.W[ell][ii,jj])\n",
    "                    self.W[ell][ii,jj] = true_W + EPS\n",
    "                    self.forward_prop(xk)\n",
    "                    l1 = self.C(self.a[-1],yk)\n",
    "                    self.W[ell][ii,jj] = true_W - EPS\n",
    "                    self.forward_prop(xk)\n",
    "                    l2 = self.C(self.a[-1],yk)\n",
    "                    num_dW = (l1 - l2) / (2*EPS)\n",
    "                    \n",
    "                    self.W[ell][ii,jj] = true_W\n",
    "                    \n",
    "                    rel_dW = np.abs(true_dW-num_dW)/np.abs(true_dW)\n",
    "                    print(\"W[{:d}][{:d},{:d}]: true: {: 12.10e}  approx: {: 12.10e} rel_err: {: 12.10e}\".format(ell, ii, jj, true_dW, num_dW, rel_dW))\n",
    "                    rel_errors.append(rel_dW)\n",
    "                                    \n",
    "                # TODO num_db  \n",
    "                true_db = self.db[ell][ii]\n",
    "                true_b = copy.deepcopy(self.b[ell][ii])\n",
    "                self.b[ell][ii] = true_b + EPS\n",
    "                self.forward_prop(xk)\n",
    "                l1 = self.C(self.a[-1],yk)\n",
    "                self.b[ell][ii] = true_b - EPS\n",
    "                self.forward_prop(xk)\n",
    "                l2 = self.C(self.a[-1],yk)\n",
    "                num_db = (l1 - l2) / (2*EPS)\n",
    "                self.b[ell][ii] = true_b\n",
    "                \n",
    "                \n",
    "                rel_db = np.abs(true_db-num_db)/np.abs(true_db)\n",
    "                print(\"b[{:d}][{:d}]:   true: {: 12.10e}  approx: {: 12.10e} rel_err: {: 12.10e}\".format(ell, ii, true_db, num_db, rel_db))\n",
    "                rel_errors.append(rel_db)\n",
    "\n",
    "        return rel_errors\n",
    "            \n",
    "            \n",
    "    def train(self, X_train, y_train, X_valid=None, y_valid=None, eta=0.25, lam=0.0, num_epochs=10, isPrint=True):\n",
    "        \"\"\"\n",
    "        Train the network with SGD \n",
    "        \n",
    "        :param X_train: matrix of training features \n",
    "        :param y_train: matrix of vector-encoded training labels \n",
    "        :param X_train: optional matrix of validation features \n",
    "        :param y_train: optional matrix of vector-encoded validation labels \n",
    "        :param eta: learning rate \n",
    "        :param lam: regularization strength \n",
    "        :param num_epochs: number of epochs to run \n",
    "        :param isPrint: flag indicating to print training progress or not \n",
    "        \"\"\"\n",
    "        \n",
    "        import copy\n",
    "        \n",
    "        # initialize shuffled indices \n",
    "        shuffled_inds = list(range(X_train.shape[0]))\n",
    "        \n",
    "        # loop over training epochs \n",
    "        for ep in range(num_epochs):\n",
    "            \n",
    "            # shuffle indices \n",
    "            np.random.shuffle(shuffled_inds)\n",
    "            \n",
    "            # loop over training examples \n",
    "            for ind in shuffled_inds:\n",
    "                \n",
    "                self.back_prop(X_train[ind],y_train[ind]) \n",
    "                dW = copy.deepcopy(self.dW)\n",
    "                sW = copy.deepcopy(self.W)\n",
    "                db = copy.deepcopy(self.db)\n",
    "                self.W = self.W - np.multiply(eta, dW) - np.multiply(sW,lam*eta)\n",
    "                self.b = self.b - np.multiply(eta, db)\n",
    "                \n",
    "            # occasionally print accuracy\n",
    "            if isPrint and ((ep+1)%5)==1:\n",
    "#                 self.epoch_report(ep, num_epochs, X_train, y_train, X_valid, y_valid)\n",
    "                self.epoch_accuracy[ep] = self.accuracy(X_valid, y_valid)\n",
    "                \n",
    "        # print final accuracy\n",
    "        if isPrint:\n",
    "            self.epoch_report(ep, num_epochs, X_train, y_train, X_valid, y_valid)\n",
    "                \n",
    "                    \n",
    "    def epoch_report(self, ep, num_epochs, X_train, y_train, X_valid, y_valid):\n",
    "        \"\"\"\n",
    "        Print the accuracy for the given epoch on training and validation data \n",
    "        \n",
    "        :param ep: the current epoch \n",
    "        :param num_epochs: the total number of epochs\n",
    "        :param X_train: matrix of training features \n",
    "        :param y_train: matrix of vector-encoded training labels \n",
    "        :param X_train: optional matrix of validation features \n",
    "        :param y_train: optional matrix of vector-encoded validation labels \n",
    "        \"\"\"\n",
    "        print(\"epoch {:3d}/{:3d}: \".format(ep+1, num_epochs), end=\"\")\n",
    "        print(\"  train acc: {:8.3f}\".format(self.accuracy(X_train, y_train)), end=\"\")\n",
    "        if X_valid is not None: \n",
    "            print(\"  valid acc: {:8.3f}\".format(self.accuracy(X_valid, y_valid)))\n",
    "            \n",
    "        else: print(\"\")  \n",
    "            \n",
    "    def get_mini_batches(self, X, y, batch_size):\n",
    "        random_idxs = random.choice(len(y), len(y), replace=False)\n",
    "        X_shuffled = X[random_idxs, :]\n",
    "        y_shuffled = y[random_idxs]\n",
    "        mini_batches = [(X_shuffled[i:i + batch_size, :], y_shuffled[i:i + batch_size]) for\n",
    "                        i in range(0, len(y), batch_size)]\n",
    "        return mini_batches\n",
    "    \n",
    "    \n",
    "    def train_nn_MBGD(self, X_train, y_train, bs=100, iter_num=30, eta=0.1, lam=0.0500):\n",
    "        # self.back_prop(X_train[0], y_train[0])\n",
    "        cnt = 0\n",
    "        m = len(y_train)\n",
    "        avg_cost_func = []\n",
    "        tri_W, tri_b = self.W, self.b\n",
    "        print('Starting gradient descent for {} iterations'.format(iter_num))\n",
    "        while cnt < iter_num:\n",
    "            if cnt % 10 == 0:\n",
    "                print('Iteration {} of {}'.format(cnt, iter_num))\n",
    "            avg_cost = 0\n",
    "            mini_batches = self.get_mini_batches(X_train, y_train, bs)\n",
    "            for mb in mini_batches:\n",
    "                X_mb = mb[0]\n",
    "                y_mb = mb[1]\n",
    "                delta_c = np.zeros(shape=(4,))\n",
    "                # pdb.set_trace()\n",
    "                for i in range(len(y_mb)):\n",
    "                    # TODO: back prop to get derivatives\n",
    "                    self.forward_prop(X_mb[i])\n",
    "\n",
    "                    # TODO: compute deltas on output layer\n",
    "\n",
    "                    delta_c = self.a[-1] - y_mb[i]\n",
    "                    de = np.multiply(delta_c, self.g_prime(self.z[-1]))\n",
    "                    self.delta[self.L - 1] = de\n",
    "                    s = 0\n",
    "                    for w in self.W:\n",
    "                        s += np.sum(np.square(w))\n",
    "                    avg_cost += np.linalg.norm(de) + (lam / (2*bs)) * s\n",
    "\n",
    "                    # TODO: loop backward through layers, backprop deltas, compute dWs and dbs\n",
    "                    for ll in range(self.L - 2, -1, -1):\n",
    "                        self.dW[ll] += np.multiply(self.delta[ll + 1][:, np.newaxis], self.a[ll][:, np.newaxis].T)\n",
    "                        self.db[ll] += self.delta[ll + 1]\n",
    "                        temp = np.dot(self.W[ll].T, self.delta[ll + 1])\n",
    "                        temp1 = self.g_prime(self.z[ll])\n",
    "                        self.delta[ll] = np.multiply(temp, temp1)\n",
    "\n",
    "                    # avg_cost += self.accuracy(X_mb, y_mb)\n",
    "                for l in range(self.L - 2, -1, -1):\n",
    "                    w = self.W\n",
    "                    self.W[l] += -eta * (self.dW[l]  + lam * w[l])\n",
    "                    self.b[l] += -eta * (self.db[l])\n",
    "\n",
    "                if ((cnt + 1) % 5) == 1:\n",
    "                    #                 self.epoch_report(ep, num_epochs, X_train, y_train, X_valid, y_valid)\n",
    "                    self.epoch_accuracy[cnt] = self.accuracy(X_mb, y_mb)\n",
    "            # complete the average cost calculation\n",
    "            avg_cost = 1.0 / m * avg_cost\n",
    "            avg_cost_func.append(avg_cost)\n",
    "            cnt += 1\n",
    "        return avg_cost_func\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting gradient descent for 30 iterations\n",
      "Iteration 0 of 30\n",
      "Iteration 10 of 30\n",
      "Iteration 20 of 30\n",
      "{0: 0.27, 5: 0.29, 10: 0.21, 15: 0.21, 20: 0.26, 25: 0.29}\n"
     ]
    }
   ],
   "source": [
    "nn = Network_leakyrelu([441,100,4])\n",
    "rel_errs = nn.train_nn_MBGD(X_train, y_train)\n",
    "# rel_errs = nn.gradient_checking(X_train, y_train)\n",
    "print(nn.epoch_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "between relu and leaky relu, leaky relu performs better for first 30 iterations because relu chops off the activation \n",
    "if z < 0. but leaky relu keeps some value if z < 0. \n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
