{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 14: The Kernel Trick and Parameter Tuning  \n",
    "***\n",
    "\n",
    "<img src=\"figs/cogs.jpg\" width=1100 height=50>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Reminder: Scroll down to the bottom and shift-enter all of the Helper Functions*\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "### Problem 1: Polynomial Kernels \n",
    "***\n",
    "\n",
    "Consider the following labeled data (with red and blue indicating the positive and negative labels, respectively)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prob1plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q**: Is this set of training data linearly separable in the traditional sense? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q**: Consider applying the quadratic kernel \n",
    "\n",
    "$$\n",
    "K({\\bf x}, {\\bf z}) = \\left({\\bf x}^T{\\bf z} + 1\\right)^2\n",
    "$$\n",
    "\n",
    "What does the derived feature vector, $\\phi({\\bf x})$, associated with this kernel look like?  You can do this by expanding the $K({\\bf x}, {\\bf z})$ in terms of the features of ${\\bf x}$ and ${\\bf z}$ and then rearranging things so it looks like the dot product $\\phi({\\bf x})^T\\phi({\\bf z})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q**: How many features are there in the higher dimensional space? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q**: Can you see a pair of features in the derived feature space that would allow the data to linearly separated? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "### Problem 2: SVMs and the Bias-Variance Trade-Off\n",
    "***\n",
    "\n",
    "Consider the standard soft-margin SVM with objective function \n",
    "\n",
    "$$\n",
    "\\min_{{\\bf w}, b} \\|{\\bf w}\\|^2 + C \\sum_{i=1}^m \\xi_i \n",
    "$$\n",
    "\n",
    "**Q**: How does the choice of **kernel** function affect the bias/variance of the model?  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q**: Similarly, for a **fixed** kernel type, how does the value of $C$ affect the bias/variance of the model? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "### Problem 3: Parameter Tuning, Accuracy, and Cross-Validation \n",
    "***\n",
    "\n",
    "Any support vector machine classifier will have at least one parameter that needs to be tuned based on the training data.  The guaranteed parameter is the $C$ that appears in front of the slack variable term in the primal objective function, i.e. \n",
    "\n",
    "$$\n",
    "\\min_{{\\bf w}, b, {\\bf \\xi}} \\frac{1}{2}\\|{\\bf w}\\|^2 + C \\sum_{i=1}^m \\xi_i\n",
    "$$\n",
    "\n",
    "If you use a kernel fancier than the linear kernel (i.e. the traditional non-kernelized SVM) then you will likely have other parameters as well. For instance in the polynomial kernel $K({\\bf x}, {\\bf z}) = ({\\bf x}^T{\\bf z} + c)^p$ you have to select the shift $c$ and the polynomial degree $p$.  Similarly the rbf kernel\n",
    "\n",
    "$$\n",
    "K({\\bf x}, {\\bf z}) = \\exp\\left[-\\gamma\\|{\\bf x} - {\\bf z}\\|^2\\right]\n",
    "$$\n",
    "\n",
    "has one tuning parameter, namely $\\gamma$, which controls how fast the similarity measure drops off with distance between ${\\bf x}$ and ${\\bf z}$. \n",
    "\n",
    "For our examples we'll consider the rbf kernel, which gives us two parameters to tune, namely $C$ and $\\gamma$. \n",
    "\n",
    "Consider the following two dimensional data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T03:20:43.685080Z",
     "start_time": "2018-03-07T03:20:43.400872Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xtrain, ytrain = prob2GenData(N=300, seed=1235)\n",
    "plotData(Xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code trains an SVM model with an rbf kernel.  Play around with the parameters $C$ and $\\gamma$ and see how this affects the decision and support vector boundaries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T03:20:48.813934Z",
     "start_time": "2018-03-07T03:20:48.461046Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = SVC(kernel=\"rbf\", C=4, gamma=3)\n",
    "plotSVM(Xtrain, ytrain, clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can decide what good parameters are, we have to know what *good* means.  One idea would be to partition your data into a training set and a **validation** set, train your SVM model on the training data, and then compute the accuracy of the model on the validation set.  Accuracy is typically computed as \n",
    "\n",
    "$$\n",
    "\\texttt{score} = \\frac{\\texttt{# correctly labeled points}}{\\texttt{# validation points}}\n",
    "$$\n",
    "\n",
    "We can easily compute this accuracy on the training data using our trained model as follows: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T03:24:15.790890Z",
     "start_time": "2018-03-07T03:24:15.787266Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Accuracy on training data = {:.3f}\".format(clf.score(Xtrain, ytrain)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, that's pretty good!  Go back and play with the parameters in the SVM classifier and see what happens to the accuracy.  Did you hone in on a good set of parameters?  \n",
    "\n",
    "OK.  So maybe you found a good set of parameters that yields a good accuracy on the training set.  But is this really what we're after?  Remember, our goal is to take our model and make predictions on future unseen data.  If we tweak our parameters with the accuracy on the training set in mind, it's likely that we will **overfit** the training data, and our model may not generalize to unseen data.  Let's generate some new data from the same distribution and see how our trained model performs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T03:24:45.476056Z",
     "start_time": "2018-03-07T03:24:45.471666Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xvalid, yvalid = prob2GenData(N=100, seed=1235)\n",
    "print(\"Accuracy on validation data = {:.3f}\".format(clf.score(Xvalid, yvalid)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, that's not too bad.  It's definite a lower accuracy score than on the training data, but we kind of expect this.  Go back and and tune the parameters some more with the goal of increasing the accuracy on the validation data instead.\n",
    "\n",
    "Were you able to do better?  If so, **COOL**!  \n",
    "\n",
    "**Q**: After parameter tuning we managed to get a pretty good accuracy on the validation data.  But can we really expect a similar accuracy on data that we'll see in the future? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Let's get a new set of completely unseen data and test the SVM model on that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T03:25:06.779022Z",
     "start_time": "2018-03-07T03:25:06.775175Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xtest, ytest = prob2GenData(N=100, seed=1241)\n",
    "print(\"Accuracy on test data = {:.3f}\".format(clf.score(Xtest, ytest)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, because we've been using simulated data we've been able to generate new data as needed on the fly.  In reality you're usually given a set of labeled training data and that's it.  You can then partition your labeled data into training, validation, and test data and run your experiments. Typically the size of the split is **60%** training data, **20%** validation data, and **20%** test data. However, other splits are common as well. \n",
    "\n",
    "What do you do if you have such little data that you can't afford to set aside 40% of it for validation and testing?  One popular solution is something called **cross-validation**.  In this case you would still set aside 20% of the data as a final test set, but you get to keep the other 20% in the training set.  A popular form of cross-validation called **K-Folds** cross-validation works as follows \n",
    "\n",
    "- Divide your training data into $K$ \"folds\" equal size sets\n",
    "- Loop over each of the folds and treat it as a hold-out set, while training on the other $K-$ folds \n",
    "- Compute the accuracy score for each of the $K$ models on the held-out data \n",
    "- Estimate the accuracy for your choice of parameters as the mean of the accuracy of the $K$ tests\n",
    "\n",
    "Let's see this in action for the example above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T03:25:13.645870Z",
     "start_time": "2018-03-07T03:25:13.631344Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = SVC(kernel='rbf', C=4, gamma=3)\n",
    "scores = cross_val_score(clf, Xtrain, ytrain, cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can either look at the individual accuracies across each of the folds: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T03:25:25.388160Z",
     "start_time": "2018-03-07T03:25:25.384225Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we generally want though is an overall accuracy score for our model with our chosen set of parameter, which we can get just by taking the mean: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T03:25:37.068833Z",
     "start_time": "2018-03-07T03:25:37.065655Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Mean Accuracy in Cross-Validation = {:.3f}\".format(scores.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "### Problem 4: Automating the Parameter Search \n",
    "***\n",
    "\n",
    "On the previous problem we were able to choose some OK parameters just by hand-tuning.  But in real life (where time is money) it would be better to do something a little more automated.  One common thing to do is a **grid-search** over a predefined range of the parameters.  In this case you will loop over all possible combinations of parameters, estimate the accuracy of your model (using, say, K-Folds cross-validation) and then choose the parameter combination that produces the highest validation accuracy. \n",
    "\n",
    "Below is an experiment where we search over a logarithmic range between $10^{-2}$ and $10^{10}$ for $C$ and a range between $10^{-9}$ and $10^{3}$ for $\\gamma$.  For the accuracy measure we use K-Folds CV with $K=3$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T03:27:01.741868Z",
     "start_time": "2018-03-07T03:25:40.755956Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "crng = np.logspace(-2,10,13)\n",
    "grng = np.logspace(-9,3,13)\n",
    "param_grid = dict(gamma=grng, C=crng)\n",
    "grid = GridSearchCV(SVC(kernel=\"rbf\"), param_grid=param_grid, cv=3)\n",
    "grid.fit(Xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function plots a heatmap that shows the validation accuracy for each parameter combination.  Which one looks the best to you? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T03:33:04.023410Z",
     "start_time": "2018-03-07T03:33:03.654981Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plotGrid(grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GridSearchSV object stores (among other things) the best combination of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T03:33:11.881698Z",
     "start_time": "2018-03-07T03:33:11.875478Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the accuracy score achieved by the best set of parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T03:28:00.650182Z",
     "start_time": "2018-03-07T03:28:00.646892Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grid.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It also stores the classifier with the best parameters, which we can pass into our plotSVM function to see the result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T03:28:07.733658Z",
     "start_time": "2018-03-07T03:28:07.333554Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plotSVM(Xtrain, ytrain, grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>\n",
    "<br><br><br><br>\n",
    "<br><br><br><br>\n",
    "\n",
    "### Helper Functions\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T03:32:55.317166Z",
     "start_time": "2018-03-07T03:32:53.825340Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.datasets import make_blobs\n",
    "from matplotlib.colors import Normalize\n",
    "%matplotlib inline\n",
    "\n",
    "def prob1plot():\n",
    "    X = np.array([[1,1], [1,-1], [-1,1], [-1,-1]])\n",
    "    y = np.array([1, -1, -1, 1])\n",
    "    mycolors = {\"blue\": \"steelblue\", \"red\": \"#a76c6e\", \"green\": \"#6a9373\"}\n",
    "    msize=200\n",
    "    fig = plt.figure(figsize=(5,5))\n",
    "    ax = fig.add_subplot(111)\n",
    "    colors = [mycolors[\"blue\"] if yi==1 else mycolors[\"red\"] for yi in y]\n",
    "    plt.scatter(X[:,0],X[:,1], marker='o', color=colors, s=msize, alpha=0.90)\n",
    "    plt.arrow(-1.25,0,2.5,0, head_length=0.05, head_width=0.05, fc=\"gray\", ec=\"gray\", lw=2, alpha=0.25)\n",
    "    plt.arrow(0,-1.25,0,2.5, head_length=0.05, head_width=0.05, fc=\"gray\", ec=\"gray\", lw=2, alpha=0.25)\n",
    "    ax.text(1.32, -.025, r\"$x_1$\", fontsize=24)\n",
    "    ax.text(-.05, 1.32, r\"$x_2$\", fontsize=24)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    plt.xticks([], fontsize=16)\n",
    "    plt.yticks([], fontsize=16)\n",
    "    \n",
    "def prob2GenData(N=100, seed=1235):\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    X = np.random.uniform(-1,1,(N,2))\n",
    "    y = np.array([1 if y-x > 0 else -1 for (x,y) in zip(X[:,0]**2 * np.sin(2*np.pi*X[:,0]), X[:,1])])\n",
    "    X = X + np.random.normal(0,.1,(N,2))\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "def plotData(X, y):\n",
    "    mycolors = {\"blue\": \"steelblue\", \"red\": \"#a76c6e\", \"green\": \"#6a9373\"}\n",
    "    msize=100\n",
    "    fig = plt.figure(figsize=(12,12))\n",
    "    ax = fig.add_subplot(111)\n",
    "    colors = [mycolors[\"blue\"] if yi==1 else mycolors[\"red\"] for yi in y]\n",
    "    plt.scatter(X[:,0],X[:,1], marker='o', color=colors, s=msize, alpha=0.25)\n",
    "    plt.arrow(-1.25,0,2.5,0, head_length=0.05, head_width=0.05, fc=\"gray\", ec=\"gray\", lw=2, alpha=0.25)\n",
    "    plt.arrow(0,-1.25,0,2.5, head_length=0.05, head_width=0.05, fc=\"gray\", ec=\"gray\", lw=2, alpha=0.25)\n",
    "    z = np.linspace(0.25,3.5,10)\n",
    "    ax.text(1.32, -.025, r\"$x_1$\", fontsize=24)\n",
    "    ax.text(-.05, 1.32, r\"$x_2$\", fontsize=24)\n",
    "    plt.xlim([-1.50,1.50])\n",
    "    plt.ylim([-1.50,1.550])\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    plt.xticks([], fontsize=16)\n",
    "    plt.yticks([], fontsize=16)\n",
    "    \n",
    "def plotSVM(X, y, clf): \n",
    "    msize=100\n",
    "    mycolors = {\"blue\": \"steelblue\", \"red\": \"#a76c6e\", \"green\": \"#6a9373\"}\n",
    "    fig = plt.figure(figsize=(12,12))\n",
    "    ax = fig.add_subplot(111)\n",
    "    colors = [mycolors[\"blue\"] if yi==1 else mycolors[\"red\"] for yi in y]\n",
    "    plt.scatter(X[:,0],X[:,1], marker='o', color=colors, s=msize, alpha=0.25)\n",
    "    plt.arrow(-1.25,0,2.5,0, head_length=0.05, head_width=0.05, fc=\"gray\", ec=\"gray\", lw=2, alpha=0.25)\n",
    "    plt.arrow(0,-1.25,0,2.5, head_length=0.05, head_width=0.05, fc=\"gray\", ec=\"gray\", lw=2, alpha=0.25)\n",
    "    z = np.linspace(0.25,3.5,10)\n",
    "    ax.text(1.32, -.025, r\"$x_1$\", fontsize=24)\n",
    "    ax.text(-.05, 1.32, r\"$x_2$\", fontsize=24)\n",
    "    plt.xlim([-1.50,1.50])\n",
    "    plt.ylim([-1.50,1.50])\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    plt.xticks([], fontsize=16)\n",
    "    plt.yticks([], fontsize=16)\n",
    "    \n",
    "    clf.fit(X,y)\n",
    "\n",
    "    x_min = X[:, 0].min()+.00\n",
    "    x_max = X[:, 0].max()-.00\n",
    "    y_min = X[:, 1].min()+.00\n",
    "    y_max = X[:, 1].max()-.00\n",
    "\n",
    "    colors = [mycolors[\"blue\"] if yi==1 else mycolors[\"red\"] for yi in y]\n",
    "\n",
    "    XX, YY = np.mgrid[x_min:x_max:200j, y_min:y_max:200j]\n",
    "    Z = clf.decision_function(np.c_[XX.ravel(), YY.ravel()])\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(XX.shape)\n",
    "    plt.contour(XX, YY, Z, colors=[mycolors[\"red\"], \"gray\", mycolors[\"blue\"]], linestyles=['--', '-', '--'],\n",
    "                levels=[-1.0, 0, 1.0], linewidths=[2,2,2], alpha=0.9)\n",
    "    \n",
    "\n",
    "# This class and the following function were blatantly ripped off \n",
    "# from the sklearn tutorial entitled RBF SVM Parameters in the \n",
    "# sklearn documentation. \n",
    "class MidpointNormalize(Normalize):\n",
    "\n",
    "    def __init__(self, vmin=None, vmax=None, midpoint=None, clip=False):\n",
    "        self.midpoint = midpoint\n",
    "        Normalize.__init__(self, vmin, vmax, clip)\n",
    "\n",
    "    def __call__(self, value, clip=None):\n",
    "        x, y = [self.vmin, self.midpoint, self.vmax], [0, 0.5, 1]\n",
    "        return np.ma.masked_array(np.interp(value, x, y))\n",
    "    \n",
    "def plotGrid(grid):\n",
    "    \n",
    "    scores = [x for x in grid.cv_results_[\"mean_test_score\"]]\n",
    "    scores = np.array(scores).reshape(len(crng), len(grng))\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.subplots_adjust(left=.2, right=0.95, bottom=0.15, top=0.95)\n",
    "    plt.imshow(scores, interpolation='nearest', cmap=plt.cm.hot,\n",
    "               norm=MidpointNormalize(vmin=0.2, midpoint=0.92))\n",
    "    plt.xlabel('gamma')\n",
    "    plt.ylabel('C')\n",
    "    plt.colorbar()\n",
    "    plt.xticks(np.arange(len(grng)), grng, rotation=45)\n",
    "    plt.yticks(np.arange(len(crng)), crng)\n",
    "    plt.title('Validation accuracy')\n",
    "    plt.show()\n",
    "    \n",
    "from IPython.core.display import HTML\n",
    "HTML(\"\"\"\n",
    "<style>\n",
    ".MathJax nobr>span.math>span{border-left-width:0 !important};\n",
    "</style>\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
