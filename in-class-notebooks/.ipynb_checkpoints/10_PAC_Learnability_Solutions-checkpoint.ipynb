{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 10: PAC Learnability \n",
    "***\n",
    "\n",
    "<img src=\"figs/cogs.jpg\" width=1100 height=50>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reminder**: *If the math type-setting looks funny, scroll down and shift-enter the single sell under Helper Functions*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "### Problem 1: PAC Learnability of Axis-Aligned Rectangles \n",
    "**Note**: This problem was adopted from Mohri, *Foundations of Machine Learning*\n",
    "***\n",
    "\n",
    "Consider the case when the input space ${\\cal X}$ is a subset of $\\mathbb{R}^2$ and the concept class $C$ is all axis-aligned rectangles lying in $\\mathbb{R}^2$ where points inside the concept rectangle are labeled as positive and points outside the rectangle are labeled as negative.  Let the hypothesis class $H$ also be the set of all axis-aligned rectangles in $\\mathbb{R}^2$.  The following image shows an example of a concept $c$ and a hypothesis $h$: \n",
    "\n",
    "\n",
    "<img src=\"figs/rectangles1.png\" width=600 height=50>\n",
    "\n",
    "\n",
    "\n",
    "In this problem you will derive a bound on the number of training examples necessary to **P**robably **A**pproximately **C**orrectly learn the target concept $c$.    \n",
    "\n",
    "**Q**: Given a set of training examples $S = \\{({\\bf x}_i, y_i)\\}_{i=1}^m$, give an algorithm that is guaranteed to return a consistent hypothesis $h$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A**: Similar to the intervals example worked out in the video, we can define a consistent hypothesis $h$ to be the smallest rectangle that contains all of the positively labeled training examples. \n",
    "\n",
    "\n",
    "<img src=\"figs/rectangles2.png\" width=600 height=50>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q**: We now want to derive a bound on the number of training examples necessary to obtain generalization error $\\epsilon > 0$ with confidence $1-\\delta$ where $\\delta > 0$.  To do this we need to put a bound on the probability that a point from the data distribution ${\\cal D}$ did not fall inside of rectangle $c$ but outside of rectangle $h$ (i.e. a miss).  Decompose the *bad* region geometrically and assign an appropriate probability that a bad example lands in each region.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A**: We can divide the region into four rectangular strips, and ascribe to each region a probability of $\\epsilon/4$ that a bad example will land in that region.  (**Bonus Question**: Why are these valid probability assignments despite the fact that the strips overlap?)\n",
    "\n",
    "\n",
    "<img src=\"figs/rectangles3.png\" width=600 height=50>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q**: By assuming that $P[R(h) > \\epsilon] < \\delta$, use your geometric argument to derive a bound on the sample size needed for $h$ to be Probably Approximately Correct.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A**: The statement that $P[R(h) > \\epsilon]$ is equivalent to the probability that no point in the training set fell into the missed region.  Thus, by the Union Bound, we have \n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "P[R(h) > \\epsilon] &=& P[\\textrm{no } {\\bf x}_i \\textrm{ fell in } r_1 \\textrm{ OR } r_2 \\textrm{ OR } r_3 \\textrm{ OR } r_4]\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "By construction, the probability that a single point did not fall in a particular region is $(1-\\epsilon/4)$, and thus the probability that none of the $m$ points fell in a particular region is $(1-\\epsilon/4)^m$.  Using the Union bound, and these probabilities, we have \n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "P[R(h) > \\epsilon] &=& P[\\textrm{no } {\\bf x}_i \\textrm{ fell in } r_1 \\textrm{ OR } r_2 \\textrm{ OR } r_3 \\textrm{ OR } r_4] \\\\\n",
    "&\\leq& \\left(1 - \\frac{\\epsilon}{4}\\right)^m + \\left(1 - \\frac{\\epsilon}{4}\\right)^m + \\left(1 - \\frac{\\epsilon}{4}\\right)^m + \\left(1 - \\frac{\\epsilon}{4}\\right)^m  \\\\\n",
    "&=& 4\\left(1 - \\frac{\\epsilon}{4}\\right)^m \\\\\n",
    "&\\leq& 4e^{-m\\epsilon/4}\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "where the last step follows from the exponential trick.  Now, we want to choose $m$ such that this upper bound on the probability that $h$ *is bad* is bounded above by $\\delta$.  We then have\n",
    "\n",
    "\n",
    "$$\n",
    "4e^{-m\\epsilon/4} < \\delta \\quad \\Rightarrow \\quad \\frac{-m\\epsilon}{4} < \\ln \\frac{\\delta}{4} \\quad \\Rightarrow \\quad m > \\frac{4}{\\epsilon}\\ln\\frac{4}{\\delta}\n",
    "$$\n",
    "\n",
    "\n",
    "Thus, for any $\\epsilon > 0$ and $\\delta > 0$ choosing $m$ greater than $(4/\\epsilon)\\ln(\\delta/4)$ ensures that $P[R(h) \\leq \\epsilon] \\geq 1 - \\delta$ \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q**: Use your bound derived above to determine a specific bound on $m$ such that a learned hypothesis is $99$% accurate $98$% of the time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A**: For 99% accuracy we choose $\\epsilon = 0.01$ and for 98% confidence we choose $\\delta = 0.02$.  Then we have\n",
    "\n",
    "\n",
    "$$\n",
    "m > \\frac{4}{.01}\\log\\frac{4}{.02} \\approx 2120\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "### Problem 2: PAC Learnability of Conjunctions of Boolean Literals\n",
    "**Note**: This problem was adopted from Mohri, *Foundations of Machine Learning*\n",
    "***\n",
    "\n",
    "Consider learning the concept class $C_n$ of at most $n$ Boolean literals $x_1, x_2, \\ldots, x_n$.  A Boolean literal is either a variable $x_i$, for $i \\in 1, \\ldots, n$ or it's negation $\\neg x_i$. For $n = 5$ an example of a conjunction of Boolean literals is $x_1 \\wedge \\neg x_3 \\wedge \\neg x_4 \\wedge x_5$.  Note that $(1,0,0,0,1)$ is a positive example for this concept while $(0,0,1,0,0)$ is a negative example. \n",
    "\n",
    "Notice that for $n=5$ if we observe a positive training example (1,0,0,1,0) tells us that the target concept cannot contain the literals $\\neg x_1$ and $\\neg x_4$ and it also cannot contain the literals $x_2$, $x_3$, and $x_5$. \n",
    "\n",
    "Notice also that if we had a negative example $(1,0,0,0,1)$ this is not as informative since we cannot tell which bits are incorrect. \n",
    "\n",
    "**Q**: Without specifying a learning algorithm, state a general bound on the number of training examples required for PAC learnability. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A**: We'll use the general bound for a consistent finite dimensional hypothesis class stated in the videos, namely \n",
    "\n",
    "$$\n",
    "m \\geq \\frac{1}{\\epsilon}\\left(\\ln\\left| H \\right| + \\ln\\frac{1}{\\delta} \\right)\n",
    "$$\n",
    "\n",
    "We need to determine the cardinality of hypothesis class $H$.  Since each of the $n$ Boolean literals can either appear, not appear, or appear negatived, there are $3^n$ possible hypotheses in $H$.  Thus we have \n",
    "\n",
    "$$\n",
    "m \\geq \\frac{1}{\\epsilon}\\left(n\\ln 3 + \\ln\\frac{1}{\\delta} \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q**: Use your bound derived above to determine a specific bound on $m$ such that a learned hypothesis is $99$% accurate $98$% of the time in the case when $n = 10$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A**: We again have $\\epsilon = 0.01$ and $\\delta = 0.02$.  Plugging in $n=10$ as well gives \n",
    "\n",
    "$$\n",
    "m \\geq \\frac{1}{.01}\\left(10\\ln 3 + \\ln\\frac{1}{.02} \\right) \\approx 1490  \n",
    "$$\n",
    "\n",
    "Thus to learn an at most length 10 Boolean literal with 99% accuracy 98% of the time we need around 1490 training examples. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've shown that the target concept $c$ is PAC learnable with $m$ bounded from below by a polynomial in $1/\\epsilon$ and $1/\\delta$. What we'd now like to show is that it is possible to learn such a concept in a reasonable amount of time.  If we can show that the hypothesis $h$ is learnable in polynomial time then we've shown that the target concept $c$ is **Efficiently** PAC Learnable. \n",
    "\n",
    "**Q**: State an algorithm that will learn a consistent hypothesis $S$ given a training set $S$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A**: Loop over all positive examples $(b_1, b_2, \\ldots, b_n)$.  If $b_i = 1$ throw out $\\neg x_i$.  If $b_i = 0$, throw out $x_i$\n",
    "\n",
    "**Example**: Suppose you have the following training data. \n",
    "\n",
    "| $x_1$ | $x_2$ | $x_3$ | $x_4$ | $x_5$ | $x_6$ | $y$ |\n",
    "|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|\n",
    "|0|1|1|0|1|1|+\n",
    "|0|1|1|1|1|1|+\n",
    "|0|0|1|1|0|1|-\n",
    "|0|1|1|1|1|1|+\n",
    "|1|0|0|1|1|0|-\n",
    "|0|1|0|0|1|1|+\n",
    "|0|1|?|?|1|1| \n",
    "\n",
    "Algorithm returns $\\neg x_1 \\wedge x_2 \\wedge x_5 \\wedge x_6$ \n",
    "\n",
    "The algorithm requires a linear scan of the positive training examples.  This means checking $n$ bits for each positive training example.  Since this complexity is polynomial in $n$ for each training example we say that the target concept is Efficiently PAC Learnable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "### Problem 3: PAC Learnability of Integer-Vertex Axis-Aligned Rectangles \n",
    "**Note**: This problem was adopted from Mitchell, *Machine Learning*\n",
    "***\n",
    "\n",
    "Consider the class of $C$ of concepts of the form $(a \\leq x \\leq b) \\wedge (c \\leq y \\leq d)$ where $a, b, c, d$ are integers in the interval $(0,99)$.  Note that each concept in the class corresponds to a rectangle with integer-valued boundaries on a portion of the $xy$-plane. \n",
    "\n",
    "\n",
    "**Q**: Give a bound on the number of training examples necessary to assure that for any target concept $c \\in C$, any consistent learner will, with probability $95$%, output a hypothesis with error at most $0.15$.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A**: Notice that there a finite number of concepts in $C$.  In the consistent case we can then use the bound \n",
    "\n",
    "$$\n",
    "m \\geq \\frac{1}{\\epsilon}\\left(\\ln\\left| H \\right| + \\ln\\frac{1}{\\delta} \\right)\n",
    "$$\n",
    "\n",
    "provided that we can calculate the cardinality of $H$.  Note that we can define an integer-vertex rectangle by combining an interval in the horizontal direction and an interval in the vertical direction (e.g. $[1,7] \\times [2,3]$ is the rectangle with lower-left vertex $(1,2)$ and upper-right vertex $(7,3)$). \n",
    "\n",
    "Unfortunately, I forgot to specify that these be nondegenerate rectangles, but we can grit our teeth and proceed.  For a single line segment we want to choose the two endpoints as a combination from the values $0,1, \\ldots, 99$ **with** replacement.  This is given by \n",
    "\n",
    "$$\n",
    "{{100+2-1}\\choose{2}} = \n",
    "{{101}\\choose{2}} = \\frac{100\\times 101}{2} = 5050\n",
    "$$\n",
    "\n",
    "Then, since each horizontal interval can be combined with each vertical interval, we have \n",
    "\n",
    "$$\n",
    "\\left| H \\right| = (5050)^2 = 25,502,500 \\quad \\Rightarrow \\quad \\log\\left| H \\right| \\approx 17.05 \n",
    "$$\n",
    "\n",
    "Then, with $\\epsilon = 0.15$ and $\\delta = 0.05$, our bound becomes \n",
    "\n",
    "$$\n",
    "m \\geq \\frac{1}{0.15}\\left(17.05 + \\ln\\frac{1}{0.05} \\right) \\approx 133.667\n",
    "$$\n",
    "\n",
    "which means we'll achieve our accuracy and confidence goals with $m \\geq 134$ training examples. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>\n",
    "<br><br><br><br>\n",
    "<br><br><br><br>\n",
    "\n",
    "### Helper Functions\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       ".MathJax nobr>span.math>span{border-left-width:0 !important};\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(\"\"\"\n",
    "<style>\n",
    ".MathJax nobr>span.math>span{border-left-width:0 !important};\n",
    "</style>\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
