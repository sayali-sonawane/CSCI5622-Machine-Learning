{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 2: Naive Bayes\n",
    "***\n",
    "\n",
    "<img src=\"files/figs/bayes.jpg\" width=1201 height=50> \n",
    "\n",
    "<!---\n",
    "![my_image](files/figs/bayes.jpg)\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1: Naive Bayes on Symbols\n",
    "***\n",
    "\n",
    "> This problem was adopted from [Naive Bayes and Text Classification I: Introduction and Theory](https://arxiv.org/abs/1410.5329) by Sebastian Raschka"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following training set of 12 symbols which have been labeled as either + or -: \n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"figs/shapes.png\"  width=500>\n",
    "\n",
    "Answer the following questions: \n",
    "\n",
    "\n",
    "**A**: What are the general features associated with each training example? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: The two general types of features are **shape** and **color**.  For this particular training set, the observed features are *shape* $\\in$ {*square*, *circle*} and *color* $\\in$ {*red*, *blue*, *green*}. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next part, we'll use Naive Bayes to classify the following test example: \n",
    "\n",
    "<img src=\"figs/bluesquare.png\"  width=200>\n",
    "\n",
    "OK, so this symbol actually appears in the training set, but let's pretend that it doesn't.  \n",
    "\n",
    "The decision rule can be defined as \n",
    "\n",
    ">Classify ${\\bf x}$ as + if <br>\n",
    ">$p(+ ~|~ {\\bf x} = [blue,~ square]) \\geq p(- ~|~ {\\bf x} = [blue, ~square])$ <br>\n",
    ">else classify sample as -\n",
    "\n",
    "**B**: What are the Maximum Likelihood Estimates of the priors $p(+)$ and $p(-)$? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: There are 12 symbols in the training set with 7 of them labeled + and 5 of them labeled -, so we have\n",
    "\n",
    "$$\n",
    "\\hat{p}(+) = \\frac{7}{12} ~~~ \\textrm{and} ~~~ \\hat{p}(-) = \\frac{5}{12} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**C**: Identify and compute estimates of the class-conditional probabilities required to predict the class of ${\\bf x} = [blue,~square]$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: The class-conditional probabilities required to classify ${\\bf x} = [blue, ~square]$ are \n",
    "\n",
    "$$\n",
    "p(blue ~|~ +), ~~~~~ p(blue ~|~ -), ~~~~~ p(square ~|~ +), ~~~~~ p(square ~|~ -)\n",
    "$$\n",
    "\n",
    "From the training set, we have \n",
    "\n",
    "$$\n",
    "\\hat{p}(blue ~|~ +)= \\frac{3}{7}, ~~~~~ \\hat{p}(blue ~|~ -) = \\frac{3}{5}, ~~~~~ \\hat{p}(square ~|~ +)=\\frac{5}{7}, ~~~~~ \\hat{p}(square ~|~ -) = \\frac{3}{5}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**D**: Using the estimates computed above, compute the **posterior** scores for each label, and find the Naive Bayes prediction of the label for ${\\bf x} = [blue,~square]$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: We have \n",
    "\n",
    "$$\n",
    "\\hat{p}(+ ~|~ {\\bf x} = [blue,~square]) = \\hat{p}(blue ~|~ +) \\cdot \\hat{p}(square ~|~ +) \\cdot \\hat{p}(+) = \\frac{3}{7} \\cdot \\frac{5}{7} \\cdot \\frac{7}{12} = 0.18 \n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{p}(- ~|~ {\\bf x} = [blue,~square]) = \\hat{p}(blue ~|~ -) \\cdot \\hat{p}(square ~|~ -) \\cdot \\hat{p}(-) = \\frac{3}{5} \\cdot \\frac{3}{5} \\cdot \\frac{5}{12} = 0.15\n",
    "$$\n",
    "\n",
    "Since $0.18 > 0.15$,  our Naive Bayes classifier would classify ${\\bf x} = [blue, ~square]$ as + "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**E**: If you haven't already, compute the class-conditional probabilities scores $\\hat{p}({\\bf x} = [blue,~square] ~|~ +)$ and $\\hat{p}({\\bf x} = [blue,~square] ~|~ -)$ under the Naive Bayes assumption.  How can you reconsile these values with the final prediction that would made? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: The class-conditional probability scores under the Naive Bayes assumption are \n",
    "\n",
    "$$\n",
    "\\hat{p}({\\bf x} = [blue,~square] ~|~ +) = \\hat{p}(blue ~|~ +) \\cdot \\hat{p}(square ~|~ +)  = \\frac{3}{7} \\cdot \\frac{5}{7} = 0.31\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{p}({\\bf x} = [blue,~square] ~|~ -) = \\hat{p}(blue ~|~ -) \\cdot \\hat{p}(square ~|~ -) = \\frac{3}{5} \\cdot \\frac{3}{5} = 0.36\n",
    "$$\n",
    "\n",
    "The - label actually has a higher class-conditional probability for ${\\bf x}$ than the + label.  We ended up predicting the + label because the prior for + was larger than the prior for -.  This example demonstrates how the choice of prior can have a large influence on the prediction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2: Laplace Smoothing \n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the same training set from Problem 2, but suppose we see the following test example: \n",
    "    \n",
    "<img src=\"figs/greencircle.png\"  width=200>\n",
    "\n",
    "Before you get too far into trying to predict the label of the green circle, look carefully at the training set.  Notice that there are no green shapes labeled - in the training set, so when we try to compute the class-conditional probability $p(green ~|~ -)$ we'll get a zero probability.  To fix this, you'll implement Laplace smoothing. Notice that this is a little different than the SPAM vs HAM example shown in the video.  We actually have two very different features in shapes and colors. We'll apply Laplace Smoothing to the shape and color class-conditional probabilities separately. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A**: What would the general formula for the estimate of $p(shape ~|~ class)$ with Laplace Smoothing look like for the given training set?  What is the *vocabulary* in the shape case?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: Since the training set only includes the shapes {*square*, *circle*}.  This is the vocabulary.  Since there are two things in the vocabulary, the estimate of $p(shape ~|~ class)$ becomes \n",
    "\n",
    "$$\n",
    "p(shape ~|~ class) = \\frac{\\#~instances~of~shape~in~class + 1}{\\#~total~symbols~in~class + 2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B**: What would the general formula for the estimate of $p(color ~|~ class)$ with Laplace Smoothing look like for the given training set?  What is the *vocabulary* in the shape case?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: Since the training set only includes the colors {*red*, *green*, *blue*}.  This is the vocabulary.  Since there are three things in the vocabulary, the estimate of $p(color ~|~ class)$ becomes \n",
    "\n",
    "$$\n",
    "p(color ~|~ class) = \\frac{\\#~instances~of~color~in~class + 1}{\\#~total~symbols~in~class + 3}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**C**: Predict the label for the green circle using the Laplaced smoothed class-conditional probability formulas.  Don't forget to apply Laplace Smoothing to the priors as well! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: The posterior scores under the Naive Bayes assumption are \n",
    "\n",
    "$$\n",
    "\\hat{p}(+ ~|~ {\\bf x} = [green,~circle]) = \\hat{p}(green ~|~ +) \\cdot \\hat{p}(circle ~|~ +) \\cdot \\hat{p}(+) = \\frac{2+1}{7+3} \\cdot \\frac{2+1}{7+2} \\cdot \\frac{7+1}{12+2}= 0.057\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{p}(- ~|~ {\\bf x} = [green,~circle]) = \\hat{p}(green ~|~ -) \\cdot \\hat{p}(circle ~|~ -) \\cdot \\hat{p}(-) = \\frac{0+1}{5+3} \\cdot \\frac{2+1}{5+2} \\cdot \\frac{5+1}{12+2}= 0.023\n",
    "$$\n",
    "\n",
    "Since $0.057 > 0.023$ we again predict + for the label. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3: Unknown Features\n",
    "***\n",
    "\n",
    "Once again consider the training set from Problem 2, but suppose we see the following test example: \n",
    "    \n",
    "<img src=\"figs/yellowsquare.png\"  width=200>\n",
    "\n",
    "OK, this is a weird one.  Up until this point, we've never seen the color *yellow*, and thus don't include it in the color vocabulary.  One way that we could handle this is to add to the color vocabulary, and then recompute the the class-conditional probabilities with *yellow* included in the vocabulary. \n",
    "\n",
    "But what happens when on the next test example we see a *pink* circle (or worse, a triangle)? We'd rather not continue to modify our probability estimates whenever we see shape or color that we haven't see before.  One solution to this is to just assume we'll see weird things in the future and combine all of the posibilities into an UNK feature. If we do this, then our class-conditional probabilities become \n",
    "\n",
    "$$\n",
    "p(feature ~|~ class) = \\frac{\\#~instances~of~feature~in~class + 1}{\\#~total~symbols~in~class + |V| + 1}\n",
    "$$\n",
    "\n",
    "where here the vocabular $V$ is the same vocabular defined by the training set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A**: Predict the label of the yellow square.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: The posterior scores under the Naive Bayes assumption are \n",
    "\n",
    "$$\n",
    "\\hat{p}(+ ~|~ {\\bf x} = [yellow,~square]) = \\hat{p}(yellow ~|~ +) \\cdot \\hat{p}(square ~|~ +) \\cdot \\hat{p}(+) = \\frac{0+1}{7+3+1} \\cdot \\frac{5+1}{7+2+1} \\cdot \\frac{7+1}{12+2}= 0.031\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{p}(- ~|~ {\\bf x} = [yellow,~square]) = \\hat{p}(yellow ~|~ -) \\cdot \\hat{p}(square ~|~ -) \\cdot \\hat{p}(-) = \\frac{0+1}{5+3+1} \\cdot \\frac{3+1}{5+2+1} \\cdot \\frac{5+1}{12+2}= 0.024\n",
    "$$\n",
    "\n",
    "Since $0.031 > 0.024$ we again predict + for the label. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>\n",
    "<br><br><br><br>\n",
    "<br><br><br><br>\n",
    "<br><br><br><br>\n",
    "### Helper Functions \n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(\"\"\"\n",
    "<style>\n",
    ".MathJax nobr>span.math>span{border-left-width:0 !important};\n",
    "</style>\n",
    "\"\"\")\n",
    "\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
