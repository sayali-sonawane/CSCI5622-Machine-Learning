{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 4: Stochastic Gradient Descent Solutions\n",
    "***\n",
    "\n",
    "<img src=\"figs/mountains.jpg\" width=1100 height=50>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "### Problem 1: Unregularized Stochastic Gradient Ascent on Text Data \n",
    "***\n",
    "\n",
    "Suppose you have two messages, the first labeled positive (y = 1) and the second labeled negative (y = 0):\n",
    "\n",
    "| POS Message | NEG Message  | \n",
    "|:-----------:|:------------:|\n",
    "| AAAABBBC    |  BBCCCD      |\n",
    "\n",
    "**Q**: Your first task is to encode the messages into feature vectors using the Bag-of-Words approach.  You may assume that \"A\", \"B\", \"C\", and \"D\" are the  only symbols in the vocabulary.  Don't forget to add a feature corresponding to the **bias** term!  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A**: Labeling the POS message ${\\bf x}_1$ and the NEG message ${\\bf x}_2$ we have $\n",
    "{\\bf x}_1 = \n",
    "\\left[\n",
    "\\begin{array}\n",
    "~ 1 \\\\\n",
    "4 \\\\\n",
    "3 \\\\\n",
    "1 \\\\\n",
    "0\n",
    "\\end{array}\n",
    "\\right]\n",
    "~~ \\textrm{and} ~~\n",
    "{\\bf x}_2 = \n",
    "\\left[\n",
    "\\begin{array}\n",
    "~ 1 \\\\\n",
    "0 \\\\\n",
    "2 \\\\\n",
    "3 \\\\\n",
    "1 \\\\\n",
    "\\end{array}\n",
    "\\right]\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q**: Your next task is to do one full pass through the data with **unregularized** SGA with the initial weight vector ${\\bf w}$ set to zero and the learning rate set to $\\eta = 1.0$.  You may assume that after the shuffle the order is POS then NEG.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A**: Working only with ${\\bf x}_1$ we have \n",
    "\n",
    "$$\n",
    "y_1 = 1 \\quad \\quad \\textrm{sigm}({\\bf w}^T{\\bf x}_1) = \\frac{1}{1 + \\exp[-(0 + 0 + 0 + 0 + 0)]} = \\frac{1}{2}  \\quad \\quad [\\textrm{sigm}({\\bf w}^T{\\bf x})-y_1] = \\frac{1}{2}-1 = -\\frac{1}{2}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "\\nabla_{\\bf w}NLL({\\bf w}) = \\left[ \\textrm{sigm}({\\bf w}^T{\\bf x}_1)-y_1\\right]{\\bf x}_1 = -\\frac{1}{2}\n",
    "\\left[\n",
    "\\begin{array}\n",
    "~ 1 \\\\\n",
    "4 \\\\\n",
    "3 \\\\\n",
    "1 \\\\\n",
    "0\n",
    "\\end{array}\n",
    "\\right]\n",
    "= \n",
    "\\left[\n",
    "\\begin{array}{r}\n",
    "-0.5 \\\\\n",
    "-2 \\\\\n",
    "-1.5 \\\\\n",
    "-0.5 \\\\\n",
    "0\n",
    "\\end{array}\n",
    "\\right]\n",
    "\\quad \\Rightarrow \\quad \n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "{\\bf w} \\leftarrow {\\bf w} - \\eta \\nabla_{\\bf w}NLL({\\bf w}) = \n",
    "\\left[\n",
    "\\begin{array}\n",
    "~ 0 \\\\\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "0\n",
    "\\end{array}\n",
    "\\right]\n",
    "- 1.0 \n",
    "\\left[\n",
    "\\begin{array}{r}\n",
    "-0.5 \\\\\n",
    "-2 \\\\\n",
    "-1.5 \\\\\n",
    "-0.5 \\\\\n",
    "0\n",
    "\\end{array}\n",
    "\\right]\n",
    "= \n",
    "\\left[\n",
    "\\begin{array}\n",
    "~ 0.5 \\\\\n",
    "2 \\\\\n",
    "1.5 \\\\\n",
    "0.5 \\\\\n",
    "0\n",
    "\\end{array}\n",
    "\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now working with ${\\bf x}_2$ we have \n",
    "\n",
    "$$\n",
    "y_2 = 0 \\quad \\quad \\textrm{sigm}({\\bf w}^T{\\bf x}_2) = \\frac{1}{1 + \\exp[-(1\\cdot 0.5 + 0\\cdot 2 + 2 \\cdot 1.5 + 3 \\cdot 0.5 + 1\\cdot 0)]} = 0.993  \n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "[\\textrm{sigm}({\\bf w}^T{\\bf x})-0] = 0.993-0 = 0.993\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "\\nabla_{\\bf w}NLL({\\bf w}) = \\left[ \\textrm{sigm}({\\bf w}^T{\\bf x}_2)-y_2\\right]{\\bf x}_2 = 0.993 \n",
    "\\left[\n",
    "\\begin{array}\n",
    "~ 1 \\\\\n",
    "0 \\\\\n",
    "2 \\\\\n",
    "3 \\\\\n",
    "1\n",
    "\\end{array}\n",
    "\\right]\n",
    "= \n",
    "\\left[\n",
    "\\begin{array}{r}\n",
    "~ 0.993 \\\\\n",
    "0 \\\\\n",
    "1.987 \\\\\n",
    "2.980 \\\\\n",
    "0.993\n",
    "\\end{array}\n",
    "\\right]\n",
    "\\quad \\Rightarrow \\quad \n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "{\\bf w} \\leftarrow {\\bf w} - \\eta \\nabla_{\\bf w}NLL({\\bf w}) = \n",
    "\\left[\n",
    "\\begin{array}\n",
    "~ 0.5 \\\\\n",
    "2.0 \\\\\n",
    "1.5 \\\\\n",
    "0.5 \\\\\n",
    "0\n",
    "\\end{array}\n",
    "\\right]\n",
    "- 1.0 \n",
    "\\left[\n",
    "\\begin{array}{r}\n",
    "0.993 \\\\\n",
    "0 \\\\\n",
    "1.987 \\\\\n",
    "2.980 \\\\\n",
    "0.993\n",
    "\\end{array}\n",
    "\\right]\n",
    "= \n",
    "\\left[\n",
    "\\begin{array}\n",
    "~ -0.493 \\\\\n",
    "~~~~2.0 \\\\\n",
    "-0.487 \\\\\n",
    "-2.480 \\\\\n",
    "-0.993\n",
    "\\end{array}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "So after one full pass through the data SGA has updated the weights to $\n",
    "{\\bf w} = \n",
    "\\left[\n",
    "\\begin{array}\n",
    "~ -0.493 \\\\\n",
    "~~~~2.0 \\\\\n",
    "-0.487 \\\\\n",
    "-2.480 \\\\\n",
    "-0.993\n",
    "\\end{array}\n",
    "\\right]\n",
    "$\n",
    "\n",
    "$\\square$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2: A More Efficient Regularization \n",
    "***\n",
    "\n",
    "Recall that we commonly include in the log-likelihood function a regularization term (on all weights except for the bias) of the form $\\lambda \\sum_{k=1}^D w_k^2$.   Incidentally, there are many forms of regularization.  This one is called L2-regularization because it can also be written as $\\lambda\\|{\\bf w}_{1:D}\\|_2^2$ where the norm here is the 2-norm or the *Euclidean* norm. For SGA this has the effect of including a term like $2\\lambda w_k$ in the $k^\\textrm{th}$ component of the gradient. \n",
    "\n",
    "**Q**: Why might this be terribly inefficient, especially in the context of text learning? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A**: Remember that the features in text learning with the Bag-of-Words model are all of the words in the vocabulary.  Since most documents will not have anywhere near the number of words in the vocabulary, feature vectors in text learning tend to be **very** sparse.  Furthermore, since $\\nabla_{\\bf w}NLL({\\bf w})$ is effectively a multiple of the feature vector ${\\bf x}_i$, it will have the same sparsity pattern (i.e. still a ton of zeros).  This means that in unregularized SGA there is no need to update the weight vector corresponding to features that are zero in ${\\bf x}_i$  (we often call these **zero-dimensions** or **zero-features**). \n",
    "\n",
    "Now, when we add regularization to the mix, it complicates matters.  The features affected by regularization are the features where the **weight vector** is nonzero.  By the end of training a Logistic Regression model on a large document corpus, a great many entries in the weight vector will likely be nonzero. If we include regularization in the update, we'll have to update tremendously more features in ${\\bf w}$ than just the nonzero features from ${\\bf x}_i$ at each step.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A workaround for this is to use a *slightly* different form of L2 regularization, which we now describe.  \n",
    "\n",
    "Consider the update step for the $k^\\textrm{th}$ component of the weight vector in SGA with regularization.  It looks like\n",
    "\n",
    "$$\n",
    "w_k \\leftarrow w_k - \\eta\\left([\\textrm{sigm}({\\bf w}^T{\\bf x}_i)-y_i]x_{ik} + 2\\lambda w_k \\right)\n",
    " = (w_k - \\eta[\\textrm{sigm}({\\bf w}^T{\\bf x}_i)-y_i]x_{ik}) - 2\\eta\\lambda w_k \n",
    "$$\n",
    "\n",
    "We're going to break the update up into two parts.  First we'll do the unregularized gradient update for $k$ corresponding only to **nonzero-features** in ${\\bf x}_i$: \n",
    "\n",
    "$$\n",
    "w_k \\leftarrow w_k - \\eta[\\textrm{sigm}({\\bf w}^T{\\bf x}_i)-y_i]x_{ik}\n",
    "$$\n",
    "\n",
    "And then (in theory) we'll do the regularization update as follows \n",
    "\n",
    "$$\n",
    "w_k \\leftarrow w_k - 2\\eta \\lambda w_k = w_k (1 - 2\\eta \\lambda)  \n",
    "$$\n",
    "\n",
    "**Q**: Look carefully at the two update steps.  Why is this not the same as standard L2-regularization? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**A**: If this was standard L2-regularization the two $w_k$'s that appear on the right-hand side of the second update step would be different.  The first instance of $w_k$ is the one that's been updated with the log-likelihood gradient and the second one should technically be the un-updated value.  In this version of regularization we're regularizing on the already updated value of $w_k$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, so far this doesn't look very helpful.  It seems like we still need to do the second update step for nonzero-weights, even if the $k^\\textrm{th}$ feature is a zero-feature. It turns out that we can actually hold off on doing the regularization update for a weight until the time when we have to do a gradient update as well.  \n",
    "\n",
    "The property of the update that makes this possible is that the regularization update is **multiplicative**.  This means that doing **three** updates of the form \n",
    "\n",
    "$$\n",
    "\\begin{array}{c}\n",
    "~ w_k \\leftarrow w_k (1 - 2\\eta \\lambda) \\\\\n",
    "~ w_k \\leftarrow w_k (1 - 2\\eta \\lambda) \\\\\n",
    "~ w_k \\leftarrow w_k (1 - 2\\eta \\lambda) \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "is equivalent to \n",
    "\n",
    "$$\n",
    "w_k \\leftarrow w_k (1 - 2\\eta \\lambda)^3\n",
    "$$\n",
    "\n",
    "This means that we can hold off on doing the regularization update for the $k^\\textrm{th}$ weight until we encounter a training example that has a $k^{\\textrm{th}}$ nonzero feature.  The regularization update is then performed where the **shrinkage factor** is raised to the power of the number of iterations since the feature was last updated. \n",
    "\n",
    "The only mechanism we need to do this is a data structure to keep track of the last iteration that a particular component of the weight vector was updated.  Typically this is done with a hash table.  In your next homework assignment you'll use a dictionary. \n",
    "\n",
    "This processes is called **Lazy Sparse Regularization** and you can read more about it in a slightly different context <a href=\"https://lingpipe.files.wordpress.com/2008/04/lazysgdregression.pdf\">here</a>. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q**: Redo Problem 1 with Lazy Sparse Regularization with $\\lambda = \\frac{1}{4}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A**: The first gradient update step for example ${\\bf x}_1$ does not change.  So after the gradient update but before regularization we have: \n",
    "\n",
    "\n",
    "$$\n",
    "{\\bf w} = \n",
    "\\left[\n",
    "\\begin{array}\n",
    "~ 0.5 \\\\\n",
    "~~2 \\\\\n",
    "1.5 \\\\\n",
    "0.5 \\\\\n",
    "~~ 0\n",
    "\\end{array}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "The shrinkage factor is $(1 - 2\\eta \\lambda) = 0.5$.  Now, in applying the regularization, remember we don't regularize the bias term, and we don't (right now) regularize weights associated with zero-features, which in this case is $k = 4$ (associated with letter D).  So the new update looks as follows (remember, we're indexing by zero here): \n",
    "\n",
    "$$\n",
    "\\begin{array}{rcccl}\n",
    "w_1 & \\leftarrow & 2 \\cdot 0.5 & = & 1 \\\\\n",
    "w_2 & \\leftarrow & 1.5 \\cdot 0.5 & = & 0.75 \\\\\n",
    "w_3 & \\leftarrow & 0.5 \\cdot 0.5 & = & 0.25 \\\\\n",
    "\\end{array}\n",
    "\\quad \\Rightarrow \\quad \n",
    "{\\bf w} = \n",
    "\\left[\n",
    "\\begin{array}\n",
    "~ 0.5 \\\\\n",
    "~~~1 \\\\\n",
    "0.75 \\\\\n",
    "0.25 \\\\\n",
    "~~~0\n",
    "\\end{array}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "OK, so now we have a different weight vector, and thus a different gradient update than we had in problem 1.  We have \n",
    "\n",
    "$$\n",
    "y_2 = 0, \\quad \\quad [\\textrm{sigm}({\\bf w}^T{\\bf x}_2)-y_2] = [\\textrm{sigm}(.5 \\cdot 1 + 1 \\cdot 0 + 0.75 \\cdot 2 + 0.25 \\cdot 3 + 0.25 0 \\cdot 1)-0] = 0.940 \n",
    "$$\n",
    "\n",
    "Performing the gradient update on **only** the features that are nonzero, we have \n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{array}{rcccl}\n",
    "w_0 & \\leftarrow & 0.5  - 0.940 \\cdot 1 & = & -0.440 \\\\\n",
    "w_2 & \\leftarrow & 0.75 - 0.940 \\cdot 2 & = & -1.13 \\\\\n",
    "w_3 & \\leftarrow & 0.25 - 0.940 \\cdot 3 & = & -2.57 \\\\\n",
    "w_4 & \\leftarrow & 0  - 0.940 \\cdot 1 & = & -0.940 \\\\\n",
    "\\end{array}\n",
    "\\quad \\Rightarrow \\quad \n",
    "{\\bf w} = \n",
    "\\left[\n",
    "\\begin{array}\n",
    "~-0.440 \\\\\n",
    "~~~~1 \\\\\n",
    "-1.13 \\\\\n",
    "-2.56 \\\\\n",
    "-0.940\n",
    "\\end{array}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "\n",
    "Now we perform the regularization.  Three things to keep in mind: $w_0$ is the bias weight, so we will not regularize it.  $w_1$ corresponds to a zero-feature in ${\\bf x}_2$ so we will not regularize it **now**.  Finally, $w_4$ was not regularized on the previous iteration, so this time we will regularize it twice.  \n",
    "\n",
    "$$\n",
    "\\begin{array}{rcccl}\n",
    "w_2 & \\leftarrow & -1.13  \\cdot 0.5 & = & -0.565 \\\\\n",
    "w_3 & \\leftarrow & -2.57  \\cdot 0.5 & = & -1.285 \\\\\n",
    "w_4 & \\leftarrow & -0.940 \\cdot (0.5)^2 & = & -0.235 & \n",
    "\\end{array}\n",
    "\\quad \\Rightarrow \\quad \n",
    "{\\bf w} = \n",
    "\\left[\n",
    "\\begin{array}{r}\n",
    "-0.440 \\\\\n",
    "~~~~1 \\\\ \n",
    "-0.565 \\\\\n",
    "-1.285 \\\\\n",
    "-0.235 & \n",
    "\\end{array}\n",
    "\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>\n",
    "<br><br><br><br><br>\n",
    "<br><br><br><br><br>\n",
    "<br><br><br><br><br>\n",
    "### Helper Functions\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       ".MathJax nobr>span.math>span{border-left-width:0 !important};\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(\"\"\"\n",
    "<style>\n",
    ".MathJax nobr>span.math>span{border-left-width:0 !important};\n",
    "</style>\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
