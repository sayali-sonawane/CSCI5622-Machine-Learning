{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 10: PAC Learnability \n",
    "***\n",
    "\n",
    "<img src=\"figs/cogs.jpg\" width=1100 height=50>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reminder**: *If the math type-setting looks funny, scroll down and shift-enter the single cell under Helper Functions*\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "### Problem 1: PAC Learnability of Axis-Aligned Rectangles \n",
    "**Note**: This problem was adopted from Mohri, *Foundations of Machine Learning*\n",
    "***\n",
    "\n",
    "Consider the case when the input space ${\\cal X}$ is a subset of $\\mathbb{R}^2$ and the concept class $C$ is all axis-aligned rectangles lying in $\\mathbb{R}^2$ where points inside the concept rectangle are labeled as positive and points outside the rectangle are labeled as negative.  Let the hypothesis class $H$ also be the set of all axis-aligned rectangles in $\\mathbb{R}^2$.  The following image shows an example of a concept $c$ and a hypothesis $h$: \n",
    "\n",
    "\n",
    "<img src=\"figs/rectangles1.png\" width=600 height=50>\n",
    "\n",
    "\n",
    "\n",
    "In this problem you will derive a bound on the number of training examples necessary to **P**robably **A**pproximately **C**orrectly learn the target concept $c$.    \n",
    "\n",
    "**Q**: Given a set of training examples $S = \\{({\\bf x}_i, y_i)\\}_{i=1}^m$, give an algorithm that is guaranteed to return a consistent hypothesis $h$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q**: We now want to derive a bound on the number of training examples necessary to obtain generalization error $\\epsilon > 0$ with confidence $1-\\delta$ where $\\delta > 0$.  To do this we need to put a bound on the probability that a point from the data distribution ${\\cal D}$ did not fall inside of rectangle $c$ but outside of rectangle $h$ (i.e. a miss).  Decompose the *bad* region geometrically and assign an appropriate probability that a bad example lands in each region.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q**: By assuming that $P[R(h) > \\epsilon] < \\delta$, use your geometric argument to derive a bound on the sample size needed for $h$ to be Probably Approximately Correct.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q**: Use your bound derived above to determine a specific bound on $m$ such that a learned hypothesis is $99$% accurate $98$% of the time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "### Problem 2: PAC Learnability of Conjunctions of Boolean Literals\n",
    "**Note**: This problem was adopted from Mohri, *Foundations of Machine Learning*\n",
    "***\n",
    "\n",
    "Consider learning the concept class $C_n$ of at most $n$ Boolean literals $x_1, x_2, \\ldots, x_n$.  A Boolean literal is either a variable $x_i$, for $i \\in 1, \\ldots, n$ or it's negation $\\neg x_i$. For $n = 5$ an example of a conjunction of Boolean literals is $x_1 \\wedge \\neg x_3 \\wedge \\neg x_4 \\wedge x_5$.  Note that $(1,0,0,0,1)$ is a positive example for this concept while $(0,0,1,0,0)$ is a negative example. \n",
    "\n",
    "Notice that for $n=5$ if we observe a positive training example (1,0,0,1,0) tells us that the target concept cannot contain the literals $\\neg x_1$ and $\\neg x_4$ and it also cannot contain the literals $x_2$, $x_3$, and $x_5$. \n",
    "\n",
    "Notice also that if we had a negative example $(1,0,0,0,1)$ this is not as informative since we cannot tell which bits are incorrect. \n",
    "\n",
    "**Q**: Without specifying a learning algorithm, state a general bound on the number of training examples required for PAC learnability. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q**: Use your bound derived above to determine a specific bound on $m$ such that a learned hypothesis is $99$% accurate $98$% of the time in the case when $n = 10$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've shown that the target concept $c$ is PAC learnable with $m$ bounded from below by a polynomial in $1/\\epsilon$ and $1/\\delta$. What we'd now like to show is that it is possible to learn such a concept in a reasonable amount of time.  If we can show that the hypothesis $h$ is learnable in polynomial time then we've shown that the target concept $c$ is **Efficiently** PAC Learnable. \n",
    "\n",
    "**Q**: State an algorithm that will learn a consistent hypothesis $h$ given a training set $S$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "### Problem 3: PAC Learnability of Integer-Vertex Axis-Aligned Rectangles \n",
    "**Note**: This problem was adopted from Mitchell, *Machine Learning*\n",
    "***\n",
    "\n",
    "Consider the class of $C$ of concepts of the form $(a \\leq x \\leq b) \\wedge (c \\leq y \\leq d)$ where $a, b, c, d$ are integers in the interval $(0,99)$.  Note that each concept in the class corresponds to a rectangle with integer-valued boundaries on a portion of the $xy$-plane. \n",
    "\n",
    "\n",
    "**Q**: Give a bound on the number of training examples necessary to assure that for any target concept $c \\in C$, any consistent learner will, with probability $95$%, output a hypothesis with error at most $0.15$.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>\n",
    "<br><br><br><br>\n",
    "<br><br><br><br>\n",
    "\n",
    "### Helper Functions\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       ".MathJax nobr>span.math>span{border-left-width:0 !important};\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(\"\"\"\n",
    "<style>\n",
    ".MathJax nobr>span.math>span{border-left-width:0 !important};\n",
    "</style>\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
